[{"title":"【机器学习实战】第二章 k近邻算法","date":"2018-05-13T07:37:50.550Z","path":"2018/05/13/【机器学习实战】第二章 k近邻算法/","text":"序言马上就要毕业了，研究生的方向是机器学习和深度学习最近开始学习机器学习，刷完吴恩达的视频后想着应该拿本实战书练手理论+实践才能出真知然后就开始啃这本书，现在看了三章，感觉这本书写的还是相当不错的，简单易懂，能实现一些基本的功能本人的环境是Anaconda3.5，实际运行过程中发现书中不少代码需要小改动，所以如果我的代码和书中不同，也是正常的注意：本博客并非完全照抄书上内容（太多了），只是摘选出本人觉得对学习毕竟有用的内容+一点个人的理解+保证代码是可运行的关于该书的源代码和数据集： 链接：点我 密码：rh3a 这篇博客主要介绍的是这本书的第二章: k近邻算法 2.1 k近邻算法概述 工作原理存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中数据最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k近邻算法中k的出处，通常k&lt;=20。最后选择k个最相似数据中出现次数最多的分类，作为新数据的分类。 k近邻算法 1234567891011121314def classify0(inx,dataSet,labels,k): #knn,inx是输入向量，dataSet和labels是训练集，k是knn的k dataSetSize = dataSet.shape diffMat = np.tile(inx,(dataSetSize[0],1)) - dataSet#np.tile函数是表示把inx沿着行列方向复制几次 sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1)#axis表示求每一行的和 distances = sqDistances ** 0.5 sortDistIndicies = distances.argsort() classCount = &#123;&#125; for i in range(k): label = labels[sortDistIndicies[i]] classCount[label] = classCount.get(label,0) + 1#get方法返回对应的值，若不存在返回默认值0 sortClassCount = sorted(classCount.items(),key=lambda d:d[1],reverse=True)#reverse表示从大到小 return sortClassCount[0][0] classify0用于实现k近邻算法，有4个输入参数：用于分类的输入向量是inX，输入的训练样本集为dataSet，标签向量为labels，最后的参数k表示最近邻居的数目这里关于两个数据之间距离的计算，我们采用欧几里得距离:例如数据集存在4个特征值，则点（7,6,9,4）与（1,0,0,1）之间的距离计算为：计算完所有点之间的距离后，可以对数据按照从小到大的次序排序。然后，确定前k个距离最小元素所在的主要分类，输入k总是正整数；之后将classCount字典分解为元组列表，然后使用lambda内置函数进行排序，注意这里d是一个键值对,d[1]指的就是值,reverse表示此处的排序为逆序，最后返回发生频率最高的元素标签。 2.2 示例: 使用k近邻算法改进约会网站的配对效果背景：我的朋友海伦一直使用在线约会网站寻找适合自己的约会对象，她经过总结发现她曾交往过三种类型的人：（1）不喜欢的人（2）魅力一般的人（3）极具魅力的人海伦希望我们的分类软件可以更好的帮助她将匹配对象划分到确切的分类中 海伦收集约会数据已经有一段时间了，她把这些数据存放在文本文件datingTestSet2.txt中，每个样本数据占据一行，总共有1000行。海伦的样本主要包含以下3种特征：（1）每年获得的飞行常客里程数（2）玩视频游戏所耗时间百分比（3）每周消费的冰淇淋公升数 2.2.1 准备数据:从文本文件中解析数据在将上述特征数据输入到分类器之前，必须将待处理数据的格式改变为分类器可以接受的格式。于是创建file2matrix的函数，以此来处理输入格式问题，该函数输入为文件名字符串，输出为训练样本矩阵和类标签向量。123456789101112131415def file2matrix(filename,n): #读取文件，返回特征矩阵和标签列表 file = open(filename) arrayOLines = file.readlines() numberOfLines = len(arrayOLines) returnMat = np.zeros((numberOfLines,n)) classLabelList = [] index = 0 for line in arrayOLines: line = line.strip() line = line.split('\\t') returnMat[index,:] = line[0:n] classLabelList.append(int(line[-1])) index += 1 return returnMat,classLabelList 2.2.2 分析数据：使用Matplotlib创建散点图 2.2.3 准备数据：归一化数据如果有两组特征数据：（0,20000,1.1）和（67,32000,0.1）那么他们之间的距离为：容易发现，上面方程中数字差值最大的属性对计算结果的影响最大，也即是说，每年获取的飞行常客里程数（此处定为第二个特征）对于计算结果的影响远远大于其他两个特征。但海伦认为这三个特征是同等重要的，因此我们要对数据做归一化处理。说起归一化，我们容易想到将取值范围处理到0~1或者-1~1之间，比如下面这个公式：newValue = (oldValue-min)/(max-min)于是我们可以编写出归一化函数autoNorm的代码：123456789def autoNorm(dataSet): #归一化特征 minVals = dataSet.min(0) maxVals = dataSet.max(0) ranges = maxVals - minVals m = dataSet.shape[0] normDataSet = dataSet - np.tile(minVals,(m,1)) normDataSet = normDataSet/np.tile(ranges,(m,1))#对应位置的值相除，不是矩阵除法 return normDataSet,ranges,minVals 这里解释一下np.tile(A,(n,m))函数,它的意思是把A沿着行复制n遍，沿着列复制m遍比如A=[1,2,3]，n=2，m=2，那么结果就是[1,2,3,1,2,3 1,2,3,1,2,3]，从一个1*3的矩阵变成一个2*6的矩阵 2.2.4 测试算法：作为完整程序验证分类器上节我们已经将数据做了处理，本节我们将测试分类器的效果。通常我们只提供已有数据的90%作为训练样本来训练分类器，而使用其余的10%数据去测试分类器，检测分类器的正确率；所谓的错误率就是分类器给出的错误结果次数除以测试数据的总数，完美分类器的错误率为0，而错误率为1的分类器不会给出任何正确的结果。代码里我们定义一个计数器变量，每次分类器错误地分类数据，计数器就加1，程序完成之后计数器的结果除以数据点总数就是错误率。为了测试分类器效果，创建函数datingClassTest12345678910111213141516def datingClassTest(): #测试分类器的功能 hoRatio = 0.1#测试集占数据集的比例 datingDataMat,datingLabels = file2matrix('datingTestSet2.txt',3) normMat,ranges,minVals = autoNorm(datingDataMat) m = normMat.shape[0] numTestVecs = int(m*hoRatio) errorCount = 0.0 for i in range(numTestVecs): classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:], datingLabels[numTestVecs:m],3) print (\"the classifier came back with: %d, the real answer is: %d\" % (classifierResult,datingLabels[i])) if classifierResult != datingLabels[i]: errorCount += 1.0 print (\"the total error rate is: %f\" % (errorCount/float(numTestVecs))) 注意此处数据是随机的，所以测试集连续也没关系。再输入datingClassTest()调用该函数，我们可以得到：the classifier came back with: 3, the real answer is: 3the classifier came back with: 2, the real answer is: 2the classifier came back with: 1, the real answer is: 1the classifier came back with: 1, the real answer is: 1…the classifier came back with: 2, the real answer is: 2the classifier came back with: 1, the real answer is: 1the classifier came back with: 3, the real answer is: 1the total error rate is: 0.050000分类器处理约会数据集的错误率大概是5%,这是一个不错的结果。我们可以改变hoRatio变量k的值，可以发现结果可能会大不相同。 2.2.5 使用算法：构建完整可用系统我们会给海伦一小段程序，通过改程序海伦会在约会网站上找到某个人并输入他的信息，程序会反馈回她对对方喜欢程度的预测值。于是我们可以编写出约会网站预测函数:1234567891011def classifyPerson(): #预测相亲对象的满意度 resultList = ['not at all','in small doses','in large doses'] percentTats = float(input(\"percentage oftime spent plating video games?\")) ffMiles = float(input(\"frequent flier miles earned per year?\")) iceCream = float(input(\"liters of ice cream consumed per year?\")) datingDataMat,datingLabels = file2matrix('datingTestSet2.txt',3) normMat,ranges,minVals = autoNorm(datingDataMat) inArr = np.array([ffMiles,percentTats,iceCream]) classifierResult = classify0((inArr-minVals)/ranges,normMat,datingLabels,3) print (\"You will probably like this person: \",resultList[classifierResult - 1]) 再输入classifyPerson(),依次输入10,10000,0.5,可以得到如下结果： percentage oftime spent plating video games?10frequent flier miles earned per year?10000liters of ice cream consumed per year?0.5You will probably like this person: in small doses 2.3 手写识别系统 2.3.1 准备数据：将图像转换为测试向量实际图像存储在第二章源代码的两个子目录内：trainingDigits中包含了大约2000个例子，每个例子的内容如下图所示；目录testDigits中包含了大约900个测试数据。我们使用trainingDigits中的数据训练分类器，使用目录testDigits中的数据测试分类器的效果。两组数据没有重叠。为了使用前面两个例子的分类器，我们必须将图像格式化处理为一个向量。我们将把一个32*32的二进制图像矩阵转换为1*1024的向量，这样前两节使用的分类器就可以处理这个数字图像信息了。我们先编写函数img2vector，将图像转换为向量:该函数创建1*1024的Numpy数组，然后打开给定的文件，循环读出文件的前32行，并将每行的头32个字符值存储在Numpy数组中，最后返回数组。12345678def img2vector(filename): returnVcet = np.zeros((1,1024)) fr = open(filename) for i in range(32): line = fr.readline() for j in range(32): returnVcet[0,i*32+j] = int(line[j]) return returnVcet 2.3.2 测试算法：使用k近邻算法识别手写数字上节我们已经将数据处理成分类器可以识别的格式，本节我们将这些数据输入到分类器，检测分类器的执行效果。在写入以下代码前，我们必须确保将from os import listdir写入文件的起始部分，这段代码的主要功能是从os模块中导入函数listir,他可以列出该目录下所有文件的文件名（方便遍历）123456789101112131415161718192021222324def handwritingClassTest(): hwLabels = [] trainingFileList = listdir('digits/trainingDigits')#获取该文件夹下的所有文件名字 m = len(trainingFileList) trainingMat = np.zeros((m,1024)) for i in range(m): fileNameStr = trainingFileList[i] fileStr = fileNameStr.split('.')[0] classNumStr = int(fileStr.split('_')[0]) hwLabels.append(classNumStr) trainingMat[i,:] = img2vector('digits/trainingDigits/%s' % fileNameStr) testFileList = listdir('digits/testDigits') errorCount = 0.0 mTest = len(testFileList) for i in range(mTest): fileNameStr = testFileList[i] fileStr = fileNameStr.split(',')[0] classNumStr = int(fileStr.split('_')[0]) vector = img2vector('digits/testDigits/%s' % fileNameStr) classifierResult = knn.classify0(vector,trainingMat,hwLabels,3) print (\"the classifier came back with: %d, the real answei is: %d\" % (classifierResult,classNumStr)) if classifierResult != classNumStr: errorCount += 1.0 print (\"the total error rate is: %f\" % (errorCount/float(mTest))) 再输入handwritingClassTest(),等待一会可以得到以下结果:the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4…the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the total error rate is: 0.010571 可以看到k近邻算法识别手写数字数据集，错误率仅为1.2%,可以尝试改变k的值、修改函数handwritingClassTest随机选取训练样本、改变训练样本的数目，都会对k近邻算法的错误率产生影响，感兴趣的话可以改变这些变量值，观察错误率的变化。观察以上算法可以发现实际使用时，算法的效果并不高。算法要为每个测试向量做2000次距离计算，每个距离计算包括了1024个维度浮点运算，总计要执行900次，此外还需要为测试向量准备2MB的存储空间。是否存在一种算法可以减少存储空间和计算时间的开销呢？k决策树（kd树）就是k近邻算法的优化版，可以节省大量的计算开销。 2.4 本章小结k近邻算法是分类数据最简单最有效的算法，k近邻算法必须保存全部数据集，如果训练数据集很大，必须使用大量的存储空间。此外，由于必须对数据集中每个数据计算距离值，实际使用时可能非常耗时。k近邻算法的另一个缺陷是它无法给出任何数据的基础结构信息，因此我们也无法知晓平均实例样本具有什么特征。下一章的决策树可以解决这个问题。 2.5 本章完整代码 knn.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import numpy as npimport matplotlib.pyplot as pltimport osplt.rcParams['font.sans-serif'] = ['SimHei']#用来正常显示中文标签plt.rcParams['axes.unicode_minus'] = False #用来正常显示负号def createDataset(): group = np.array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]]) labels = ['A','A','B','B'] return group,labelsdef classify0(inx,dataSet,labels,k): #knn,inx是输入向量，dataSet和labels是训练集，k是knn的k dataSetSize = dataSet.shape diffMat = np.tile(inx,(dataSetSize[0],1)) - dataSet#np.tile函数是表示把inx沿着行列方向复制几次 sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1)#axis表示求每一行的和 distances = sqDistances ** 0.5 sortDistIndicies = distances.argsort() classCount = &#123;&#125; for i in range(k): label = labels[sortDistIndicies[i]] classCount[label] = classCount.get(label,0) + 1#get方法返回对应的值，若不存在返回默认值0 sortClassCount = sorted(classCount.items(),key=lambda d:d[1],reverse=True)#reverse表示从大到小 return sortClassCount[0][0]def file2matrix(filename,n): #读取文件，返回特征矩阵和标签列表 file = open(filename) arrayOLines = file.readlines() numberOfLines = len(arrayOLines) returnMat = np.zeros((numberOfLines,n)) classLabelList = [] index = 0 for line in arrayOLines: line = line.strip() line = line.split('\\t') returnMat[index,:] = line[0:n] classLabelList.append(int(line[-1])) index += 1 return returnMat,classLabelListdef autoNorm(dataSet): #归一化特征 minVals = dataSet.min(0) maxVals = dataSet.max(0) ranges = maxVals - minVals m = dataSet.shape[0] normDataSet = dataSet - np.tile(minVals,(m,1)) normDataSet = normDataSet/np.tile(ranges,(m,1))#对应位置相除，不是矩阵除法 return normDataSet,ranges,minValsdef datingClassTest(): #测试分类器的功能 hoRatio = 0.1#测试集占数据集的比例 datingDataMat,datingLabels = file2matrix('datingTestSet2.txt',3) normMat,ranges,minVals = autoNorm(datingDataMat) m = normMat.shape[0] numTestVecs = int(m*hoRatio) errorCount = 0.0 for i in range(numTestVecs): classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:], datingLabels[numTestVecs:m],3) print (\"the classifier came back with: %d, the real answer is: %d\" % (classifierResult,datingLabels[i])) if classifierResult != datingLabels[i]: errorCount += 1.0 print (\"the total error rate is: %f\" % (errorCount/float(numTestVecs))) def classifyPerson(): #预测相亲对象的满意度 resultList = ['not at all','in small doses','in large doses'] percentTats = float(input(\"percentage oftime spent plating video games?\")) ffMiles = float(input(\"frequent flier miles earned per year?\")) iceCream = float(input(\"liters of ice cream consumed per year?\")) datingDataMat,datingLabels = file2matrix('datingTestSet2.txt',3) normMat,ranges,minVals = autoNorm(datingDataMat) inArr = np.array([ffMiles,percentTats,iceCream]) classifierResult = classify0((inArr-minVals)/ranges,normMat,datingLabels,3) print (\"You will probably like this person: \",resultList[classifierResult - 1]) #classifyPerson()#datingClassTest() find_digit.py(手写数字识别) 123456789101112131415161718192021222324252627282930313233343536373839import knnimport numpy as npfrom os import listdirdef img2vector(filename): returnVcet = np.zeros((1,1024)) fr = open(filename) for i in range(32): line = fr.readline() for j in range(32): returnVcet[0,i*32+j] = int(line[j]) return returnVcetdef handwritingClassTest(): hwLabels = [] trainingFileList = listdir('digits/trainingDigits')#获取该文件夹下的所有文件名字 m = len(trainingFileList) trainingMat = np.zeros((m,1024)) for i in range(m): fileNameStr = trainingFileList[i] fileStr = fileNameStr.split('.')[0] classNumStr = int(fileStr.split('_')[0]) hwLabels.append(classNumStr) trainingMat[i,:] = img2vector('digits/trainingDigits/%s' % fileNameStr) testFileList = listdir('digits/testDigits') errorCount = 0.0 mTest = len(testFileList) for i in range(mTest): fileNameStr = testFileList[i] fileStr = fileNameStr.split(',')[0] classNumStr = int(fileStr.split('_')[0]) vector = img2vector('digits/testDigits/%s' % fileNameStr) classifierResult = knn.classify0(vector,trainingMat,hwLabels,3) print (\"the classifier came back with: %d, the real answei is: %d\" % (classifierResult,classNumStr)) if classifierResult != classNumStr: errorCount += 1.0 print (\"the total error rate is: %f\" % (errorCount/float(mTest)))handwritingClassTest() 啊，终于写完了，真累，希望这个系列可以坚持到最后吧（逃感谢观看，最后放张灵梦镇博（笑","tags":[{"name":"机器学习实战","slug":"机器学习实战","permalink":"http://flandre.site/tags/机器学习实战/"},{"name":"机器学习","slug":"机器学习","permalink":"http://flandre.site/tags/机器学习/"}]},{"title":"重新开始更新博客","date":"2018-05-12T12:22:35.078Z","path":"2018/05/12/重新开始更新博客/","text":"很久没有好好更新过博客了，想着是时候好好更新一波了之前的博客由于图片大部分失效，想着也没什么有含金量的内容，就全删了，so，再一次从零开始本人CSDN：https://my.csdn.net/acm_cxq","tags":[{"name":"杂谈","slug":"杂谈","permalink":"http://flandre.site/tags/杂谈/"}]},{"title":"解决markdown无法显示本地图片的办法","date":"2017-12-29T12:16:31.598Z","path":"2017/12/29/解决markdown无法显示本地图片的办法/","text":"如果觉得这个方法麻烦可以试一下QQ空间相册之类的图床，应该还是不用怕被删除的。参考 http://blog.csdn.net/sugar_rainbow/article/details/57415705这里再说一下重点： 把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true 在你的hexo目录下执行这样一句话npm install hexo-asset-image –save 等待一小段时间后，再运行hexo n “xxxx”来生成md博文时，/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹当然直接在_posts文件夹里面创建一个跟你博文同名的文件夹也可以 最后在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片：注意： xxxx是这个md文件的名字，也是同名文件夹的名字。只需要有文件夹名字即可，不需要有什么绝对路径。你想引入的图片就只需要放入xxxx这个文件夹内就好了，很像引用相对路径。","tags":[{"name":"markdown","slug":"markdown","permalink":"http://flandre.site/tags/markdown/"}]}]