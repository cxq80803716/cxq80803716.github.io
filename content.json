[{"title":"【转】神经网络中的反向传播算法推导","date":"2018-09-03T07:43:46.799Z","path":"2018/09/03/【转】神经网络中的反向传播算法推导/","text":"最近看到一篇博客，把传统神经网络中的BP算法推导的很清晰明了。这里是传送门。目前还没有看懂CNN中的BP算法，等更深入理解CNN之后再来看吧。感谢分享，感谢开源（鞠躬）。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://flandre.site/tags/机器学习/"}]},{"title":"用keras实现CNN","date":"2018-09-03T06:40:43.768Z","path":"2018/09/03/用keras实现CNN/","text":"序言现在实现CNN主要采用tensorflow，但是由于tensorflow的不易上手性，所以为了更方便入门，本文采用keras实现一个CNN模型。基于被广泛使用的公共数据集mnist，实现手写数字识别。 我会将代码分成几部分，对每部分进行讲解，最后会附上完整代码。1234567from keras.datasets import mnistimport matplotlib.pyplot as pltimport randomimport numpy as npfrom keras.models import Sequentialfrom keras.layers import Dense,Conv2D,MaxPool2D,Flatten,Dropoutfrom keras.utils import np_utils 这部分主要包含了模型所需要的头文件。 12345678(X_train,y_train),(X_test,y_test) = mnist.load_data()n_train,n_test = X_train.shape[0],X_test.shape[0]X_train = X_train.reshape(X_train.shape[0],28,28,1).astype(&apos;float32&apos;)X_test = X_test.reshape(X_test.shape[0],28,28,1).astype(&apos;float32&apos;)X_train = X_train / 255X_test = X_test / 255y_train = np_utils.to_categorical(y_train)y_test = np_utils.to_categorical(y_test) 首先载入mnist数据集，注意得到的X_train是(60000,28,28)的图片，X_test是(1000,28,28)的图片，y_train和y_test是(60000,1)的标签(代表手写数字0-9)。然后将X转换一下维度，变成四维矩阵，最后一位代表了颜色，并且进行归一化。由于我们使用的CNN最后输出需要经过softmax，所以将y进行ont-hot操作。所谓ont-hot指的是把一个确定的类别转化成多个类别，但是只有一个类别是1，其他都是0。比如这个例子里一共有10个类别，有一个样本的y是3，那么转化之后就变成[0,0,0,1,0,0,0,0,0,0]。 1234567891011121314151617181920def model_init(): model = Sequential() model.add(Conv2D(filters= 16, kernel_size=(5,5), padding=&apos;Same&apos;, input_shape=(28,28,1), activation=&apos;relu&apos;)) model.add(MaxPool2D(pool_size=(2,2))) model.add(Conv2D(filters= 36, kernel_size=(3,3), padding=&apos;Same&apos;, activation=&apos;relu&apos;)) model.add(MaxPool2D(pool_size=(2,2), strides=(2,2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(128, activation=&apos;relu&apos;)) model.add(Dense(10, activation=&apos;softmax&apos;)) print (model.summary()) return model model = model_init()model.compile(optimizer=&apos;adam&apos;, loss=&apos;categorical_crossentropy&apos;, metrics=[&apos;accuracy&apos;])model.fit(X_train, y_train, epochs=5, batch_size=256, validation_split=0.2) 处理好数据，就该进行建模了。keras的建模非常简单，如同model_init函数所示。keras的model分为两种，一种是Sequential，一种是Model。使用的比较多的是前者，表示之后所有的层都是顺序连接的，即只要使用add方法即可；后者操作比较自由，可以构建多分支的神经网络，比如可以用在多标签分类问题中。Conv2D(filters= 16, kernel_size=(5,5), padding=’Same’, input_shape=(28,28,1), activation=’relu’)建立了一个卷积层，其中的卷积核(滤波器)数量是16;一个卷积核的大小是55;padding有两种模式，’Same’指的是在不足以覆盖卷积核时，自动补充0边界;activation代表了激活函数，CNN中一般使用ReLUMaxPool2D(pool_size=(2,2))建立了一个池化层，池化核的大小是22Dropout(0.25)用来按照一定概率去除神经元，防止过拟合Flatten()用来过渡卷积层和全连接层，将多维的神经元平铺成一维Dense(128, activation=’relu’)建立了一个全连接层，包含128个神经元，激活函数是ReLUmodel.summary()可以输出所建立的model的结构初始化model后，再对其进行编译，添加优化器，损失函数，评价指标。 注意添加优化器时默认学习率是0.01，也可以对其初值进行修改，甚至可以利用回调函数进行动态修改（比如验证集上准确率连续两次没有上升则降低学习率）。然后就可以开始训练模型了，传入训练集的输入和输出，epochs表示迭代几次，迭代一次会遍历整个训练集；batch_size可以设置一批的数据大小，注意我们训练数据时是按照一批一批数据训练的；validation_split表示划分多大的验证集，这里是0.2，即60000*0.2=12000条数据。那么训练集就是48000条数据。12345def evaluate(model,X_test,y_test): result = model.evaluate(np.array(X_test).reshape(len(X_test),28,28,1), y_test, batch_size=32) return result print (evaluate(model,X_test,y_test)) 用测试集对模型进行评估，返回result包括两个值，一个是loss，另一个是metrics 123456index = random.randint(0, n_test-1)y_pred = model.predict(X_test[index].reshape(1,28,28,1))plt.title(&apos;real: %s\\npred:%s&apos;%(np.argmax(y_test[index]), np.argmax(y_pred)))plt.imshow(X_test[index,:,:,0], cmap=&apos;gray&apos;)plt.axis(&apos;off&apos;)model.save(&apos;single_mnist_model.h5&apos;) 从测试集中随机抽一个数字，显示数字，预测结果和真正结果。 完整代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556from keras.datasets import mnistimport matplotlib.pyplot as pltimport randomimport numpy as npfrom keras.models import Sequentialfrom keras.layers import Dense,Conv2D,MaxPool2D,Flatten,Dropoutfrom keras.utils import np_utils#from keras.models import load_modeldef model_init(): model = Sequential() model.add(Conv2D(filters= 16, kernel_size=(5,5), padding=&apos;Same&apos;, input_shape=(28,28,1), activation=&apos;relu&apos;)) model.add(MaxPool2D(pool_size=(2,2))) model.add(Conv2D(filters= 36, kernel_size=(3,3), padding=&apos;Same&apos;, activation=&apos;relu&apos;)) model.add(MaxPool2D(pool_size=(2,2), strides=(2,2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(128, activation=&apos;relu&apos;)) model.add(Dense(10, activation=&apos;softmax&apos;)) print (model.summary()) return model def evaluate(model,X_test,y_test): # TODO: 按照错一个就算错的规则计算准确率. result = model.evaluate(np.array(X_test).reshape(len(X_test),28,28,1), y_test, batch_size=32) return result(X_train,y_train),(X_test,y_test) = mnist.load_data()n_train,n_test = X_train.shape[0],X_test.shape[0]X_train = X_train.reshape(X_train.shape[0],28,28,1).astype(&apos;float32&apos;)X_test = X_test.reshape(X_test.shape[0],28,28,1).astype(&apos;float32&apos;)X_train = X_train / 255X_test = X_test / 255y_train = np_utils.to_categorical(y_train)y_test = np_utils.to_categorical(y_test)model = model_init()model.compile(optimizer=&apos;adam&apos;, loss=&apos;categorical_crossentropy&apos;, metrics=[&apos;accuracy&apos;])model.fit(X_train, y_train, epochs=5, batch_size=256, validation_split=0.2)#model = load_model(&apos;single_mnist_model.h5&apos;)print (evaluate(model,X_test,y_test))index = random.randint(0, n_test-1)y_pred = model.predict(X_test[index].reshape(1,28,28,1))plt.title(&apos;real: %s\\npred:%s&apos;%(np.argmax(y_test[index]), np.argmax(y_pred)))plt.imshow(X_test[index,:,:,0], cmap=&apos;gray&apos;)plt.axis(&apos;off&apos;)model.save(&apos;single_mnist_model.h5&apos;)","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://flandre.site/tags/机器学习/"},{"name":"keras","slug":"keras","permalink":"http://flandre.site/tags/keras/"},{"name":"CNN","slug":"CNN","permalink":"http://flandre.site/tags/CNN/"}]},{"title":"【机器学习实战】第五章 逻辑回归","date":"2018-05-18T09:45:44.817Z","path":"2018/05/18/【机器学习实战】第五章 逻辑回归/","text":"序言本人的环境是Anaconda3.5，实际运行过程中发现书中不少代码需要小改动，所以如果我的代码和书中不同，也是正常的注意：本博客并非完全照抄书上内容（太多了），只是摘选出本人觉得对学习比较有用的内容+一点个人的理解，并且保证代码是可运行的关于该书的源代码和数据集： 链接：点我 密码：rh3a 这一章是很重要的一章，这也将是我们第一次接触到最优化算法。仔细想想就能发现，其实我们日常生活中遇到过很多最优化问题，比如如何在最短时间内从A点到达B点？如何投入最少工作量获得最大收益？关于回归，我们将在第八章深入介绍。假如现在有一些数据点，我们用一条直线对这些点进行拟合，这个过程就称作回归。 5.1 基于Logistic回归和Sigmoid函数的分类我们想要的函数应该是，能够接受所有的输入然后预测出类别。比如在两个类的情况下，上述函数能够输出0或1.或许你之前接触过这种性质的函数，这种函数被称之为海维赛德阶跃函数，或者单位阶跃函数。海维赛德阶跃函数的问题在于：该函数在跳跃点上从0瞬间跳跃到1，这个瞬间跳跃过程有时很难处理。幸好，sigmoid函数也有类似的性质，并且更容易处理。我们定义sigmoid函数为：当x=0时，sigmoid函数值为0.5，随着x的增大，对应的sigmoid值将逼近于1；随着x的减小，sigmoid值将逼近于0.如果横坐标刻度足够大，sigmoid函数看起来很想一个阶跃函数，因此，为了实现Logistic回归分类器，我们可以在每个特征上都乘以一个回归系数，然后把所有的结果值相加，将这个总和带入sigmoid中，进而得到一个0~1之间的数值。任何大于0.5的数据被分入1类，小于0.5被归入0类。确定了分类器的函数形式h = sigmoid(θ0+θ1x1+…+θnxn)之后，现在的问题变成了：最佳回归系数(θ)是多少？如何确定它们的大小？ 5.2 基于最优化方法的最佳回归系数确定我们把sigmoid函数的输入记为z，z=θ0+θ1x1+…+θnxn,采用向量的写法，定义为z=wTx 5.2.1 梯度上升（下降）法该最优化方法称为梯度上升法，基本思想是：要找到某函数的最大值，最好的办法是沿着该函数的梯度方向探寻。如果梯度记为▽，那么函数f(x,y)的梯度为：我们在高等数学中学过，梯度是变化最大的方向，这个梯度意味着要沿x的方向移动∂f(x,y)/∂x,沿y的方向移动∂f(x,y)/∂y。其中，函数f必须在待计算的点上有定义并且可微。我们定义α为学习率，也就是学习的速度，每次移动的步长。于是梯度上升算法的迭代公式为:w=w+α▽wf(w)实际上梯度上升算法就是梯度下降算法，梯度下降算法定义为w=w-α▽wf(w)，梯度上升算法用来求解函数的最大值，而梯度下降算法用来求解函数的最小值。 5.2.2 训练算法：使用梯度下降找到最佳参数下面的代码是梯度下降法的具体实现。123456789101112131415161718192021222324def loadDataSet(): dataMat = [];labelMat = [] fr = open('testset.txt') for line in fr.readlines(): lineArr = line.strip().split() dataMat.append([1.0,float(lineArr[0]),float(lineArr[1])]) labelMat.append(int(lineArr[-1])) return dataMat,labelMatdef sigmoid(inX): return 1.0/(1+np.exp(-inX))def gradDesc(dataMatIn,classLabels): dataMatrix = np.mat(dataMatIn) labelMat = np.mat(classLabels).transpose() m,n = np.shape(dataMatrix) alpha = 0.001 maxCycles = 500 weights = np.ones((n,1)) for k in range(maxCycles): h = sigmoid(dataMatrix * weights) error = (h - labelMat) weights = weights - alpha * dataMatrix.transpose() * error #① return weights 我们定义损失函数为误差平方的均值，通过求微分就可以得到①，这里不多解释。然后我们输入：12dataMat,labelMat=loadDataSet()print (gradDesc(dataMat,labelMat)) 于是可以得到：[[ 4.12414349] [ 0.48007329] [-0.6168482 ]] 5.2.3 画出决策边界现在我们要画出数据集和Logistic回归最佳拟合直线的函数123456789101112131415161718192021def plotBestFit(dataMat,labelMat,weight): m,n = np.shape(dataMat) dataMat = np.array(dataMat) xcord1 = [];ycord1 = []#类别1 xcord0 = [];ycord0 = []#类别0 for i in range(m): if int(labelMat[i]) == 1: xcord1.append(dataMat[i,1]);ycord1.append(dataMat[i,2]) else: xcord0.append(dataMat[i,1]);ycord0.append(dataMat[i,2]) fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(xcord1,ycord1,s=30,c='red',marker='s') ax.scatter(xcord0,ycord0,s=30,c='green') x = np.arange(-3.0,3.0,0.1) #x1 y = (-weights[0]-weights[1]*x)/weights[2] #z=θ0+θ1x1+θ2x2，z=0时sigmoid等于0.5,已知x1反解x2 y = y.A.flatten()#把y拍平成数组，如果用的随机梯度下降法，weights不是矩阵，y也就不是矩阵，不需要扁平 ax.plot(x,y) plt.xlabel('x1') plt.ylabel('x2') plt.show() 然后运行如下代码：123dataMat,labelMat=loadDataSet()weights = gradDesc(dataMat,labelMat)plotBestFit(dataMat,labelMat,weights) 于是可以得到： 5.2.4 训练算法：随机梯度下降梯度下降算法在每次更新回归系数时都需要遍历整个数据集，该方法在处理100个左右的数据集时尚可，但如果有数十亿样本和成千上万的特征，那么该方法的计算复杂度就太高了。一种改进办法是一次仅用一次样本点来更新回归系数，该方法称为随机梯度下降算法。由于可以在新样本到来时对分类器进行增量式更新，因此随机梯度下降算法是一个在线学习算法，与“在线学习”相对应，一次处理所有数据被称作是“批处理”随机梯度下降算法：1234567891011def stocGradAscent(dataMat,classLabels,numIter=150): m,n = np.shape(dataMat) weights = np.ones(n) for j in range(numIter): for i in range(m): alpha = 4/(1.0+j+i)+0.01#逐渐减小学习率 randIndex = int(random.uniform(0,m)) h = sigmoid(sum(dataMat[randIndex]*weights)) error = h - classLabels[randIndex] weights = weights - alpha * error * dataMat[randIndex] return weights 该代码中逐渐减小学习率的原因是在这些参数大的波动停止后，还有一些小的周期性波动，这样也方便提高收敛速度。α虽然随着迭代次数不断减小，但永远不会减小到0，这是因为还有一个常数项0.01.这样做的原因是为了保证在多次迭代之后新数据仍然具有一定的影响。另外有一点需要注意的是，α并非是严格下降的，当j远远小于max(i)时，α就有可能上升，避免参数的严格下降也常见于模拟退火算法等其他优化算法中。接下来我们看一下效果：123dataMat,labelMat=loadDataSet()weights = stocGradAscent(np.array(dataMat),labelMat)plotBestFit(dataMat,labelMat,weights) 注意此时的此时的weights已经不是矩阵是数组，所以plotBestFit函数中的y = y.A.flatten()应该注释掉然后我们可以得到： 5.3 示例：从疝气病症预测病马的死亡率（跑去百度了疝这个字怎么读..处理缺失值是一个很重要的问题，我们可以采取多种办法，比如 使用可用特征的均值来填补缺失值 使用特殊值来填补缺失值，比如-1 忽略有缺失值的样本 使用相似样本的均值填补缺失值 使用另外的机器学习算法预测缺失值这里我们选择第二点，用0来填补数据的缺失。 5.3.2 测试算法：用Logistic回归进行分类12345678910111213141516171819202122232425262728293031323334def classifyVector(inX, weights): prob = sigmoid(sum(inX*weights)) if prob &gt; 0.5: return 1.0 else: return 0.0def colicTest(): frTrain = open('horseColicTraining.txt'); frTest = open('horseColicTest.txt') trainingSet = []; trainingLabels = [] for line in frTrain.readlines(): currLine = line.strip().split('\\t') lineArr =[] for i in range(21): lineArr.append(float(currLine[i])) trainingSet.append(lineArr) trainingLabels.append(float(currLine[21])) trainWeights = stocGradAscent(np.array(trainingSet), trainingLabels, 500) errorCount = 0; numTestVec = 0.0 for line in frTest.readlines(): numTestVec += 1.0 currLine = line.strip().split('\\t') lineArr =[] for i in range(21): lineArr.append(float(currLine[i])) if int(classifyVector(np.array(lineArr), trainWeights))!= int(currLine[21]): errorCount += 1 errorRate = (float(errorCount)/numTestVec) print (\"当前测试数据的误差为: %f\" % errorRate) return errorRatedef multiTest(): numTests = 10; errorSum=0.0 for k in range(numTests): errorSum += colicTest() print (\"在数据缺失率为0.3,%d次测试下,均误差为: %f\" % (numTests, errorSum/float(numTests))) 第一个函数是分类函数，若sigmoid值大于0.5则返回1，否则返回0第二个函数是打开测试集和训练集，进行分类后返回误差第三个函数表示循环10次，求平均缺失率我们输入；1multiTest() 可以得到：当前测试数据的误差为: 0.253731当前测试数据的误差为: 0.298507当前测试数据的误差为: 0.298507当前测试数据的误差为: 0.537313当前测试数据的误差为: 0.283582当前测试数据的误差为: 0.298507当前测试数据的误差为: 0.402985当前测试数据的误差为: 0.388060当前测试数据的误差为: 0.447761当前测试数据的误差为: 0.507463在数据缺失率为0.3,10次测试下,均误差为: 0.371642从上面的结果可以看到，10次迭代之后的平均错误率为35%。，，这个结果并不是很差，因为有30%的数据缺失，如果调整迭代次数和学习率，平均错误率有可能降到20%。 5.4 本章小结逻辑回归的目的是寻找一个非线性函数sigmoid的最佳拟合参数，求解过程可以由最优化算法来完成。在最优化算法中，最常用的就是梯度下降法，而梯度下降算法又可以简化为随机梯度下降算法。随机梯度下降算法与梯度下降算法的效果相当，但占用更少的计算资源。机器学习的一个重要问题就是如何处理缺失数据，这个问题没有标准答案，取决于实际应用中的需求。 5.5 本章代码：logRegre.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100import numpy as npimport matplotlib.pyplot as pltimport randomdef loadDataSet(): dataMat = [];labelMat = [] fr = open('testset.txt') for line in fr.readlines(): lineArr = line.strip().split() dataMat.append([1.0,float(lineArr[0]),float(lineArr[1])]) labelMat.append(int(lineArr[-1])) return dataMat,labelMatdef sigmoid(inX): return 1.0/(1+np.exp(-inX))def gradDesc(dataMatIn,classLabels): dataMatrix = np.mat(dataMatIn) labelMat = np.mat(classLabels).transpose() m,n = np.shape(dataMatrix) alpha = 0.001 maxCycles = 500 weights = np.ones((n,1)) for k in range(maxCycles): h = sigmoid(dataMatrix * weights) error = (h - labelMat) weights = weights - alpha * dataMatrix.transpose() * error return weightsdef stocGradAscent(dataMat,classLabels,numIter=150): m,n = np.shape(dataMat) weights = np.ones(n) for j in range(numIter): for i in range(m): alpha = 4/(1.0+j+i)+0.01#逐渐减小学习率 randIndex = int(random.uniform(0,m)) h = sigmoid(np.sum(dataMat[randIndex]*weights)) error = h - classLabels[randIndex] weights = weights - alpha * error * dataMat[randIndex] return weights def plotBestFit(dataMat,labelMat,weight): m,n = np.shape(dataMat) dataMat = np.array(dataMat) xcord1 = [];ycord1 = []#类别1 xcord0 = [];ycord0 = []#类别0 for i in range(m): if int(labelMat[i]) == 1: xcord1.append(dataMat[i,1]);ycord1.append(dataMat[i,2]) else: xcord0.append(dataMat[i,1]);ycord0.append(dataMat[i,2]) fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(xcord1,ycord1,s=30,c='red',marker='s') ax.scatter(xcord0,ycord0,s=30,c='green') x = np.arange(-3.0,3.0,0.1) #x1 y = (-weights[0]-weights[1]*x)/weights[2]#z=θ0+θ1x1+θ2x2，z=0时sigmoid等于0.5,已知x1反解x2 y = y.A.flatten()#把y拍平成数组，如果用的随机梯度下降法，weights不是矩阵，y也就不是矩阵，不需要扁平 ax.plot(x,y) plt.xlabel('x1') plt.ylabel('x2') plt.show()def classifyVector(inX, weights): prob = sigmoid(np.sum(inX*weights)) if prob &gt; 0.5: return 1.0 else: return 0.0def colicTest(): frTrain = open('horseColicTraining.txt'); frTest = open('horseColicTest.txt') trainingSet = []; trainingLabels = [] for line in frTrain.readlines(): currLine = line.strip().split('\\t') lineArr =[] for i in range(21): lineArr.append(float(currLine[i])) trainingSet.append(lineArr) trainingLabels.append(float(currLine[21])) trainWeights = stocGradAscent(np.array(trainingSet), trainingLabels, 500) errorCount = 0; numTestVec = 0.0 for line in frTest.readlines(): numTestVec += 1.0 currLine = line.strip().split('\\t') lineArr =[] for i in range(21): lineArr.append(float(currLine[i])) if int(classifyVector(np.array(lineArr), trainWeights))!= int(currLine[21]): errorCount += 1 errorRate = (float(errorCount)/numTestVec) print (\"当前测试数据的误差为: %f\" % errorRate) return errorRatedef multiTest(): numTests = 10; errorSum=0.0 for k in range(numTests): errorSum += colicTest() print (\"在数据缺失率为0.3,%d次测试下,均误差为: %f\" % (numTests, errorSum/float(numTests))) multiTest() 后记这一章相对比较简单，下一章进入支持向量机，难度将会大大提升","tags":[{"name":"机器学习实战","slug":"机器学习实战","permalink":"http://flandre.site/tags/机器学习实战/"},{"name":"机器学习","slug":"机器学习","permalink":"http://flandre.site/tags/机器学习/"}]},{"title":"【机器学习实战】第四章 朴素贝叶斯","date":"2018-05-14T12:30:14.432Z","path":"2018/05/14/【机器学习实战】第四章 朴素贝叶斯/","text":"序言本人的环境是Anaconda3.5，实际运行过程中发现书中不少代码需要小改动，所以如果我的代码和书中不同，也是正常的注意：本博客并非完全照抄书上内容（太多了），只是摘选出本人觉得对学习比较有用的内容+一点个人的理解，并且保证代码是可运行的关于该书的源代码和数据集： 链接：点我 密码：rh3a 前面两章我们要求分类器明确给出数据实例应该属于哪一类，不过，分类器有时候会产生错误结果，这时可以要求分类器给出一个最优的类别猜测，并且给出这个猜测的概率估计值。本章主要讲解的就是基于概率论的分类方法——朴素贝叶斯 4.1 基于贝叶斯决策理论的分类方法优点： 在数据较少的情况下仍然有效，可以处理多类别问题。缺点： 对于输入数据的准备方式较为敏感。适用数据类型： 标称型数据 朴素贝叶斯是贝叶斯决策理论的一部分，所以讲述朴素贝叶斯之前有必要快速了解一下贝叶斯决策理论。如果有两个类别，现在有一个数据实例属于类别1的概率是A，属于类别2的概率是B，那么：① 如果A&gt;B，那么该数据实例的类别是1② 如果A&lt;B，那么该数据实例的类别是2也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。 4.2 条件概率这节我们将会稍微讲解一下概率论的基础知识之一条件概率P(A|B)表示在B发生的情况下，A发生的概率，这也是条件概率的定义，并且有P(A|B) = P(AB) / P(B)基于该公式，我们可以得出贝叶斯准则，贝叶斯准则告诉我们如何交换条件概率中的条件与结果，贝叶斯准则可以表示为： 4.3 使用条件概率来分类4.1节提到贝叶斯决策理论要求计算两个概率p1和p2: 如果p1&gt;p2,那么属于类别1 如果p1&lt;p2,那么属于类别2假设ci是第i个类别，那么由贝叶斯准则可以得到：基于此定义，可以定义贝叶斯分类准则为: 如果P(c1|x,y) &gt; P(c2|x,y),那么属于类别c1 如果P(c1|x,y) &lt; P(c2|x,y),那么属于类别c2在已知输入特征的情况下，我们会选择概率最大的分类作为预测分类 4.4 使用朴素贝叶斯进行文档分类机器学习的一个重要应用就是文档的自动分类。在文档分类中，整个文档（比如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。虽然电子邮件是一种会不断增加的文本，但我们同样也可以对新闻报道、用户留言、政府公文等其他任意类型的文本进行分类。我们可以观察文档中出现的词，并把每个词的出现或者不出现作为一个特征，这样得到的特征数目就会跟词汇表中的词目一样多。朴素贝叶斯是上节介绍的贝叶斯分类器的一个拓展，是用于文档分类的常用算法。使用每个词作为特征并观察他们是否出现，这样得到的特征数目会有多少呢？针对的是哪一种语言呢？仅仅在英语中，单词总数就有500000之多。为了能够进行英语阅读，至少需要掌握数千单词。朴素指的是特征之间相互独立，所谓独立指的是统计学意义上的独立，即一个特征或单词出现的可能性与其他单词没有关系。这是一个很强的假设，一般是实现不了的，比如bacon常常出现在delicious附近，而很少出现在unhealthy附近，虽然如此，朴素贝叶斯的效果往往不错。 4.5 使用Python进行文本分类 4.5.1 准备数据：从文本中构建词向量我们将把文本看成单词向量或者词条向量，也就是说将句子转换为向量。考虑所有文本中出现的单词，再决定将那些词纳入词汇表，然后必须把每一篇文档转换为词汇表上的向量。接下来我们创建一个叫bayes.py的新文件。123456789101112131415161718192021222324252627def loadDataSet(): #读取数据集 postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] classVec = [0,1,0,1,0,1] #1代表该言论有侮辱性，0代表正常言论 return postingList,classVecdef createVocabList(dataSet): #创建训练集中出现的所有不重复单词组成的列表 vocabSet = set([]) #创建一个空集 for document in dataSet: vocabSet = vocabSet | set(document) #集合的与操作 return list(vocabSet)def setOfWords2Vec(vocabList,inputSet): #将输入向量映射到vocabList，存在则对应值为1 returnVec = [0] * len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] = 1 else: print ('单词%s不在词汇列表里' % word) return returnVec 第一个函数loadDataSet创建了一些实验样本，该函数返回的第一个变量是进行此条切分的文档集合，这些文档来自斑点犬爱好者留言板。第二个变量是一个类别标签的集合，0表示非侮辱性言论，1表示侮辱性言论。第二个函数createVocabList会创建一个包含所有文档中出现的不重复词的列表，为此使用了Python的set用于去重，|表示求解集合的并集。第三个函数setOfWords2Vec用于把所有文档转换成词汇表上的向量，0表示不存在该单词，1表示存在该单词。然后我们测试一下，输入1234567listOPosts,listClasses = loadDataSet()myVocabList = createVocabList(listOPosts)print (myVocabList)print (listOPosts[0])print (setOfWords2Vec(myVocabList,listOPosts[0]))print (listOPosts[3])print (setOfWords2Vec(myVocabList,listOPosts[3])) 于是我们可以得到：[‘food’, ‘posting’, ‘steak’, ‘please’, ‘has’, ‘garbage’, ‘problems’, ‘dog’, ‘not’, ‘stop’, ‘ate’, ‘love’, ‘to’, ‘help’, ‘quit’, ‘maybe’, ‘him’, ‘my’, ‘I’, ‘dalmation’, ‘buying’, ‘take’, ‘stupid’, ‘cute’, ‘worthless’, ‘mr’, ‘park’, ‘licks’, ‘flea’, ‘so’, ‘how’, ‘is’][‘my’, ‘dog’, ‘has’, ‘flea’, ‘problems’, ‘help’, ‘please’][0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0][‘stop’, ‘posting’, ‘stupid’, ‘worthless’, ‘garbage’][0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0] 4.5.2 训练算法：从词向量计算概率前面介绍了如何将一组单词转换为一组数字，接下来看看如何使用这些数字计算概率。我们重写贝叶斯准则，将之前的x,y替换为w。w表示这是一个向量，即它由多个特征组成。于是有我们将使用上述公式，对每个类计算该值，然后比较这两个概率值的大小，如何计算呢？首先可以通过类别i(侮辱性留言或者非侮辱性留言)的文档数除以总的文档数来计算概率P(ci)，接下来计算P(w|ci),这里就要用到朴素贝叶斯假设。如果将w展开为一个个独立特征，那么就可以将上述概率写作P(w0,w1,…,wn|ci)。这里假设所有词都相互独立，该假设也称作条件独立性假设，它意味着可以使用P(w0|ci)P(w1|ci)…P(wn|ci)来计算上述概率，这就极大地简化了计算的过程。123456789101112131415161718import numpy as npdef trainNO0(trainMatrix,trainCategory): #训练朴素贝叶斯分类器，参数分贝时训练集和其对应的类别 numTrainDocs = len(trainMatrix)#训练集的数据个数 numWords = len(trainMatrix[0])#每个数据的单词个数，其实是数据集中出现的所有不重复单词个数 pAbusive = sum(trainCategory) / numTrainDocs#类别为1的数据个数占数据集的比例 p0Num = np.zeros(numWords);p1Num = np.zeros(numWords)#类别为0和1时每个特征为1的个数 p0Denom = 0.0;p1Denom = 0.0;#类别是0和1时出现的特征总数 for i in range(numTrainDocs): if trainCategory[i] == 1: p1Num += trainMatrix[i] p1Denom += sum(trainMatrix[i]) else: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) p1Vect = p1Num/p1Denom #在类别为1的情况下，每个特征出现的概率 p0Vect = p0Num/p0Denom #在类别是0的情况下，每个特征出现的概率 return p0Vect,p1Vect,pAbusive 然后我们可以试验一下这个函数:123456789listOPosts,listClasses = loadDataSet()myVocabList = createVocabList(listOPosts)trainMat = []for postinDoc in listOPosts: trainMat.append(setOfWords2Vec(myVocabList,postinDoc))p0V,p1V,pAb = trainNO0(trainMat,listClasses)print (p0V)print (p1V)print (pAb) 于是我们可以得到：[-3.25809654 -3.25809654 -2.56494936 -2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936 -2.56494936 -2.56494936 -2.56494936 -3.25809654 -3.25809654 -2.15948425 -1.87180218 -2.56494936 -2.56494936 -3.25809654 -3.25809654 -3.25809654 -2.56494936 -3.25809654 -2.56494936 -3.25809654 -2.56494936 -2.56494936 -2.56494936 -2.56494936 -2.56494936][-2.35137526 -2.35137526 -3.04452244 -3.04452244 -3.04452244 -2.35137526 -3.04452244 -1.94591015 -2.35137526 -2.35137526 -3.04452244 -3.04452244 -2.35137526 -3.04452244 -2.35137526 -2.35137526 -2.35137526 -3.04452244 -3.04452244 -3.04452244 -2.35137526 -2.35137526 -1.65822808 -3.04452244 -1.94591015 -3.04452244 -2.35137526 -3.04452244 -3.04452244 -3.04452244 -3.04452244 -3.04452244]0.5分析一下这个结果：首先我们发现文档属于侮辱类的概率pAb为0.5，该值是正确的。在非侮辱性留言中，概率最大的是-1.87180218，是第18个词汇，我们可以发现这个词汇是my，这是一个合理的结果；在侮辱性留言中，概率最大的是-1.65822808，是第23个单词，我们可以发现这个单词是stupid，显然这个结果也是合理的,这也意味着stupid是最能表征类别1的单词。在使用这个函数进行分类之前，我们还需解决函数中的一些缺陷。 4.5.3 测试算法：根据现实情况修改分类器利用朴素贝叶斯分类器进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，如果其中一个概率为0，则最后的结果也会是0。这显然不是我们想要的结果，为了解决这个问题，我们可以将所有词的出现数初始化为1，并将分母初始化为2。另一个遇到的问题是下溢出，这是由于太多很小的数相乘造成的，我们知道概率是一个属于[0,1]的话，如果特征比较多，结果将会非常小。一种解决方法是对乘积取自然对数，我们知道ln(ab)=lna + lnb，于是通过求对数可以避免下溢出或者浮点数舍入导致的错误。通过，采用自然对数进行处理不会有任何损失，这是因为lnx（x&gt;0）是一个单调递增的函数，于是x越大，lnx便越大，并且可以在相同点取到极值。用不用自然对数虽然结果并不同，但不影响最终结果。于是我们可以将训练函数修改为：123456789101112131415161718192021def trainNO0(trainMatrix,trainCategory): #训练朴素贝叶斯分类器，参数分贝时训练集和其对应的类别 numTrainDocs = len(trainMatrix)#训练集的数据个数 numWords = len(trainMatrix[0])#每个数据的单词个数，其实是数据集中出现的所有不重复单词个数 pAbusive = sum(trainCategory) / numTrainDocs#类别为1的数据个数占数据集的比例 p0Num = np.ones(numWords);p1Num = np.ones(numWords)#类别为0和1时每个特征为1的个数 p0Denom = 2.0;p1Denom = 2.0;#类别是0和1时出现的特征总数,这两行代码这么改是为了减少某个概率为0出现的影响# p0Num = np.zeros(numWords);p1Num = np.zeros(numWords)#类别为0和1时每个特征为1的个数# p0Denom = 0.0;p1Denom = 0.0;#类别是0和1时出现的特征总数 for i in range(numTrainDocs): if trainCategory[i] == 1: p1Num += trainMatrix[i] p1Denom += sum(trainMatrix[i]) else: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) p1Vect = np.log(p1Num/p1Denom) #在类别为1的情况下，每个特征出现的概率 p0Vect = np.log(p0Num/p0Denom) #在类别是0的情况下，每个特征出现的概率# p1Vect = p1Num/p1Denom #在类别为1的情况下，每个特征出现的概率# p0Vect = p0Num/p0Denom #在类别是0的情况下，每个特征出现的概率 return p0Vect,p1Vect,pAbusive 然后我们就可以编写分类函数了：1234567891011121314151617181920212223def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1): #用朴素贝叶斯分类器进行分类，第一个参数是输入向量，后面三个就是trainNO0返回的三个值 p1 = sum(vec2Classify * p1Vec) + np.log(pClass1) p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1) if p1 &gt; p0: return 1 return 0def testingNB(): #测试分类器 postingList,classVec = loadDataSet() vocabList = createVocabList(postingList) trainMat = [] for postinDoc in postingList: trainMat.append(setOfWords2Vec(vocabList,postinDoc)) p0V,p1V,pAb = trainNO0(trainMat,classVec) testEntry = ['love','my','dalmation'] thisDoc = np.array(setOfWords2Vec(vocabList,testEntry)) print (testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)) testEntry = ['stupid','garbage'] thisDoc = np.array(setOfWords2Vec(vocabList,testEntry)) print (testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)) 下面我们可以测试一下结果，输入：print (testingNB())于是可以得到：[‘love’, ‘my’, ‘dalmation’] classified as: 0[‘stupid’, ‘garbage’] classified as: 1 4.6 示例：使用朴素贝叶斯过滤垃圾邮件 4.6.1 准备数据：切分文本对于一个文本字符串，可以使用Python的String.split()方法将其划分，还可以使用正则表达式来提取单词并且当文本中出现URL的时候，诸如answer.py?hl=en&amp;answer=174623会出现en和py这样无意义的单词，所以我们将去掉长度小于3的单词。于是可以编写如下简单代码：123def textParse(bigString): listOfTokens = re.findall(r'\\w+',bigString) #用正则表达式去分割字符串，r表示原始字符串，不考虑转义 return [tok.lower() for tok in listOfTokens if len(tok) &gt; 2] #过滤掉字母少于3个的单词并将所有单词转换为小写 4.6.2 测试算法：使用朴素贝叶斯进行交叉验证12345678910111213141516171819202122232425262728293031def spamTest(): #对贝叶斯垃圾邮件分类器进行自动化处理 docList = [];classList = []; for i in range(1,26): wordList = textParse(open(&apos;email/spam/%d.txt&apos; % i,errors=&apos;ignore&apos;).read()) #print (wordList) docList.append(wordList) classList.append(1) wordList = textParse(open(&apos;email/ham/%d.txt&apos; % i,errors=&apos;ignore&apos;).read()) #print (wordList) docList.append(wordList) classList.append(0) vocabList = bayes.createVocabList(docList) trainingSet = [i for i in range(0,50)];testSet = [] for i in range(10):#随机选十个数据做测试集 randIndex = int(random.uniform(0,len(trainingSet))) testSet.append(randIndex) del(trainingSet[randIndex]) trainMat = [];trainClasses = [] for docIndex in trainingSet: trainMat.append(bayes.setOfWords2Vec(vocabList,docList[docIndex])) trainClasses.append(classList[docIndex]) p0V,p1V,pSpam = bayes.trainNO0(np.array(trainMat),np.array(trainClasses)) errorCount = 0 for docIndex in testSet: wordVector = bayes.setOfWords2Vec(vocabList,docList[docIndex]) if bayes.classifyNB(np.array(wordVector),p0V,p1V,pSpam) != classList[docIndex]: errorCount += 1 error = float(errorCount)/len(testSet) print (&apos;错误率是： &apos;, error) return error 该函数对贝叶斯垃圾邮件分类器进行自动化处理。导入文件夹spam和ham下的文本文件，并将它们解析为词列表。接下来构建一个测试集与一个训练集，两个集合中的邮件都是随机选出的。这种随机选择数据的一部分作为训练集，剩下的作为测试集的过程称为留存交叉验证。假定现在只完成了一次迭代，那么为了更精确地估计分类器的错误率，就应该进行多次迭代后求出平均错误率。接下来对该函数进行测试：12345sum = 0.0for i in range(10): sum += spamTest()avg = sum / 10.0print ('平均错误率是： ',avg) 可以得到结果：错误率是： 0.0错误率是： 0.0错误率是： 0.0错误率是： 0.0错误率是： 0.1错误率是： 0.1错误率是： 0.0错误率是： 0.0错误率是： 0.0错误率是： 0.0平均错误率是： 0.02这是一个相当不错的结果。 4.7 本章完整代码bayes.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import numpy as npdef loadDataSet(): #读取数据集 postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] classVec = [0,1,0,1,0,1] #1代表该言论有侮辱性，0代表正常言论 return postingList,classVecdef createVocabList(dataSet): #创建训练集中出现的所有不重复单词组成的列表 vocabSet = set([]) #创建一个空集 for document in dataSet: vocabSet = vocabSet | set(document) #集合的与操作 return list(vocabSet)def setOfWords2Vec(vocabList,inputSet): #将输入向量映射到vocabList，存在则对应值为1 returnVec = [0] * len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] = 1 else: print ('单词%s不在词汇列表里' % word) return returnVecdef bagOfWords2VecMN(vocabList,inputSet): #将输入向量映射到vocabList，存在则对应值非0，并且记录出现次数 returnVec = [0] * len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] += 1 else: print ('单词%s不在词汇列表里' % word) return returnVecdef trainNO0(trainMatrix,trainCategory): #训练朴素贝叶斯分类器，参数分贝时训练集和其对应的类别 numTrainDocs = len(trainMatrix)#训练集的数据个数 numWords = len(trainMatrix[0])#每个数据的单词个数，其实是数据集中出现的所有不重复单词个数 pAbusive = sum(trainCategory) / numTrainDocs#类别为1的数据个数占数据集的比例 p0Num = np.ones(numWords);p1Num = np.ones(numWords)#类别为0和1时每个特征为1的个数 p0Denom = 2.0;p1Denom = 2.0;#类别是0和1时出现的特征总数,这两行代码这么改是为了减少某个概率为0出现的影响# p0Num = np.zeros(numWords);p1Num = np.zeros(numWords)#类别为0和1时每个特征为1的个数# p0Denom = 0.0;p1Denom = 0.0;#类别是0和1时出现的特征总数 for i in range(numTrainDocs): if trainCategory[i] == 1: p1Num += trainMatrix[i] p1Denom += sum(trainMatrix[i]) else: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) p1Vect = np.log(p1Num/p1Denom) #在类别为1的情况下，每个特征出现的概率 p0Vect = np.log(p0Num/p0Denom) #在类别是0的情况下，每个特征出现的概率# p1Vect = p1Num/p1Denom #在类别为1的情况下，每个特征出现的概率# p0Vect = p0Num/p0Denom #在类别是0的情况下，每个特征出现的概率 return p0Vect,p1Vect,pAbusivedef classifyNB(vec2Classify,p0Vec,p1Vec,pClass1): #用朴素贝叶斯分类器进行分类，第一个参数是输入向量，后面三个就是trainNO0返回的三个值 p1 = sum(vec2Classify * p1Vec) + np.log(pClass1) p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1) if p1 &gt; p0: return 1 return 0def testingNB(): #测试分类器 postingList,classVec = loadDataSet() vocabList = createVocabList(postingList) trainMat = [] for postinDoc in postingList: trainMat.append(setOfWords2Vec(vocabList,postinDoc)) p0V,p1V,pAb = trainNO0(trainMat,classVec) testEntry = ['love','my','dalmation'] thisDoc = np.array(setOfWords2Vec(vocabList,testEntry)) print (testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)) testEntry = ['stupid','garbage'] thisDoc = np.array(setOfWords2Vec(vocabList,testEntry)) print (testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))#print (testingNB()) email.py123456789101112131415161718192021222324252627282930313233343536373839404142434445import bayesimport re,randomimport numpy as npdef textParse(bigString): listOfTokens = re.findall(r'\\w+',bigString) #用正则表达式去分割字符串，r表示原始字符串，不考虑转义 return [tok.lower() for tok in listOfTokens if len(tok) &gt; 2]#过滤掉字母少于3个的单词并将所有单词转换为小写def spamTest(): #对贝叶斯垃圾邮件分类器进行自动化处理 docList = [];classList = []; for i in range(1,26): wordList = textParse(open('email/spam/%d.txt' % i,errors='ignore').read()) #print (wordList) docList.append(wordList) classList.append(1) wordList = textParse(open('email/ham/%d.txt' % i,errors='ignore').read()) #print (wordList) docList.append(wordList) classList.append(0) vocabList = bayes.createVocabList(docList) trainingSet = [i for i in range(0,50)];testSet = [] for i in range(10):#随机选十个数据做测试集 randIndex = int(random.uniform(0,len(trainingSet))) testSet.append(randIndex) del(trainingSet[randIndex]) trainMat = [];trainClasses = [] for docIndex in trainingSet: trainMat.append(bayes.setOfWords2Vec(vocabList,docList[docIndex])) trainClasses.append(classList[docIndex]) p0V,p1V,pSpam = bayes.trainNO0(np.array(trainMat),np.array(trainClasses)) errorCount = 0 for docIndex in testSet: wordVector = bayes.setOfWords2Vec(vocabList,docList[docIndex]) if bayes.classifyNB(np.array(wordVector),p0V,p1V,pSpam) != classList[docIndex]: errorCount += 1 error = float(errorCount)/len(testSet) print ('错误率是： ', error) return errorsum = 0.0for i in range(10): sum += spamTest()avg = sum / 10.0print ('平均错误率是： ',avg) 后记：给博客添加了评论功能，虽然百度还是没有收录","tags":[{"name":"机器学习实战","slug":"机器学习实战","permalink":"http://flandre.site/tags/机器学习实战/"},{"name":"机器学习","slug":"机器学习","permalink":"http://flandre.site/tags/机器学习/"}]},{"title":"【机器学习实战】第三章 决策树","date":"2018-05-13T13:33:53.940Z","path":"2018/05/13/【机器学习实战】第三章 决策树/","text":"序言本人的环境是Anaconda3.5，实际运行过程中发现书中不少代码需要小改动，所以如果我的代码和书中不同，也是正常的注意：本博客并非完全照抄书上内容（太多了），只是摘选出本人觉得对学习比较有用的内容+一点个人的理解，并且保证代码是可运行的关于该书的源代码和数据集： 链接：点我 密码：rh3a 这篇博客主要介绍的是这本书的第三章: 决策树 如果你以前没有接触过决策树，完全不用担心，它的概念非常简单。如下图就是一个决策树，分支节点就是决策用的某一个特征值，叶子结点表示已经得出结果。可想而知，当我们构造出这样一颗决策树，对于一个未知结果的输入向量，能够很快得到预测结果。专家系统中经常使用决策树，而且决策树给出结果往往可以匹敌在当前领域具有几十年工作经验的人类专家。决策树的优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。缺点：可能会产生过度匹配问题使用数据类型：数值型和标称型 3.1 决策树的构造在构造决策树时，我们需要解决的第一个问题就是当前数据集上哪个特征在划分数据分类时起决定性作用，也就是说选择这个特征值可以使决策树更小。为了找到决定性的特征，划分出最好的结果，我们必须评估每一个特征。完成测试之后，原始数据集就被划分成几个数据子集。这些数据子集会分布在第一个决策点的所有分支上。如果某个分支下的数据属于同一类型，则当前已经正确划分数据分类；无需进一步对数据集进行分割。如果数据子集内的数据不属于同一类型，则需要重复划分数据子集。划分数据子集的算法和划分原始数据集的方法相同，直到所有具有相同类型的数据均在一个数据子集内。 3.1.1 信息增益划分数据集的大原则是：将无序的数据变得更加有序。我们可以使用多种方法划分数据集，但是每种方法都有各自的优缺点。组织杂乱无章数据的一种方法就是使用信息论度量信息。在划分数据集之前之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。什么是熵？熵定义为信息的期望值，在明晰这个概念之前，我们必须知道信息的定义。如果待分类的事物可能划分在多个分类之中，则符号xi的信息定义为其中p(xi)是选择分类xi的概率，显然信息量的取值范围在(0,+∞)为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，可以通过下面的公式得到：其中n是分类的数目，从公式可以看出这个信息熵相当于所有分类的期望熵，由此公式我们可以计算给定数据集的熵，代码如下：12345678910111213141516from math import logdef calcShannonEnt(dataSet): #计算数据集的香农熵，表征混乱程度 numEntries = len(dataSet) labelCounts = &#123;&#125;#该数据集所有分类出现的次数 for featVec in dataSet: currentLabel = featVec[-1] if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 labelCounts[currentLabel] += 1 shannonEnt = 0.0 for key in labelCounts: prob = float(labelCounts[key])/numEntries shannonEnt -= prob * log(prob,2) return shannonEnt 为了测试这个函数的功能，我们可以利用createDataSet()函数得到简单的鱼类鉴定数据集12345def createDataSet(): #创建数据集 dataSet = [[1,1,'yes'],[1,1,'yes'],[1,0,'no'],[0,1,'no'],[0,1,'no']] labels = ['no surfacing','flippers']#两个特征的含义 return dataSet,labels 再输入：123myDat,labels = createDataSet()print (myDat)print (calcShannonEnt(myDat)) 我们可以得到如下结果：[[1, 1, ‘yes’], [1, 1, ‘yes’], [1, 0, ‘no’], [0, 1, ‘no’], [0, 1, ‘no’]]0.9709505944546686熵越高，则混合的数据也越多，数据更加无序。得到熵之后，我们就可以按照获取最大信息增益的方法划分数据集，下一节我们将具体学习如何划分数据集以及如何度量信息增益。 3.1.2 划分数据集上节我们学习了如何度量数据集的无序程度。我们将对每个特征划分数据集的结果计算一次信息熵，然后判断按照哪个特征划分数据集是最好的划分方式。123456789def splitDataSet(dataSet,axis,value): #返回第axis个特征为value的的数据集，要去掉第axis个特征 retDataSet = [] for featVec in dataSet: if featVec[axis] == value: reducedFeatVec = featVec[:axis] reducedFeatVec.extend(featVec[axis+1:]) retDataSet.append(reducedFeatVec) return retDataSet 以上代码使用了三个输入参数：待划分的数据集、目标特征的下标，目标特征。注意区分append和extend的区别假如a = [1,2,3],b = [4,5,6]则a.append(b)的结果是[1,2,3,[4,5,6]]a.extend(b)的结果是[1,2,3,4,5,6]我们可以在前面的简单样本数据上测试该函数输入:1234myDat,labels = createDataSet()print (myDat)print (splitDataSet(myDat,0,1))print (splitDataSet(myDat,0,0)) 我们可以得到：[[1, 1, ‘yes’], [1, 1, ‘yes’], [1, 0, ‘no’], [0, 1, ‘no’], [0, 1, ‘no’]][[1, ‘yes’], [1, ‘yes’], [0, ‘no’]][[1, ‘no’], [1, ‘no’]]我们便用第一个特征对原数据集进行了划分，分成了两个子集接下来我们将遍历整个数据集，循环计算香农熵和splitDataSet()函数，找到最好的特征划分方式。123456789101112131415161718def chooseBestFeatureToSplit(dataSet): #选择当前分类的最佳特征，即信息增益最大的特征 numFeature = len(dataSet[0]) - 1 baseEntropy = calcShannonEnt(dataSet) bestInfoGain = 0.0;bestFeature = -1 for i in range(numFeature):#遍历所有特征 featList = [example[i] for example in dataSet] UniqueFeatList = set(featList) newEntropy = 0.0 for value in UniqueFeatList:#遍历该特征的所有取值，求出平均香农熵 subDataSet = splitDataSet(dataSet,i,value) prob = len(subDataSet) / float(len(dataSet)) newEntropy += prob * calcShannonEnt(subDataSet) infoGain = baseEntropy - newEntropy if infoGain &gt; bestInfoGain:#松弛操作 bestInfoGain = infoGain bestFeature = i return bestFeature 以上代码实现选取特征，划分数据集，计算得出最好的划分数据集的特征。在函数中调用的数据需要满足一定的要求，第一个要求是dataSet必须是一种由列表元素组成的列表，而且所有的列表元素都必须具有相同的数据长度；第二个要求是数据的最后一列或者说每个实例的最后一个元素是当前实例的类别标签（也就是结果）。数据集只要满足上述要求，我们就可以在函数的第一行判定当前数据集包含多少特征属性。我们无需指定lsit中的数据类型，它们既可以是数字也可以是字符串，并不影响实际计算。在开始划分数据集之前，第三行代码计算机整个数据集的原始香农熵，我们保存最初的无需度量值，用于与划分完之后的数据集计算的熵值进行比较。set函数用于去重。遍历当前特征中的所有的唯一属性值，对每个唯一属性划分一次数据集，然后计算数据集的新熵值，并对所有唯一特征值得到的熵求和。最后比较所有特征中的信息增益，返回最好特征划分的索引值。接下来我们稍微测试一下，输入下列命令：1234myDat,labels = createDataSet()feat = chooseBestFeatureToSplit(myDat)print (feat)print (myDat) 然后我们可以得到：0[[1, 1, ‘yes’], [1, 1, ‘yes’], [1, 0, ‘no’], [0, 1, ‘no’], [0, 1, ‘no’]]运行结果告诉我们，第0个特征是最好的用于第一次划分数据集的特征。观察可以看出这个结果是符合的。 3.1.3 递归构建决策树目前我们已经学习了从数据集构造决策树算法所需要的子功能模块，其工作原理如下：得到原始数据集，然后基于最好的特征划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。因此我们可以才用递归处理数据集。递归结束的条件是：程序遍历完所有划分数据集的特征，或者每个分支下的所有实例都具有相同的分类。如果所有实例具有相同的分类，则得到一个叶子结点或者终止块。任何到达叶子节点的数据必须属于叶子结点所属的分类。如果数据集已经处理了所有的特征，但是类标签仍然不是唯一的，此时我们需要决定如何定义该叶子结点，在这种情况下，我们通常会采用多数表决的方法决定该叶子节点的分类。123456789def majorityCnt(classList): #当所有特征都分类完，但是结果仍不唯一时，选择出现次数最多的类别 classCount = &#123;&#125; for vote in classList: if vote not in classCount.keys(): classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount.items(),key=lambda d:d[1],reversed=True) return sortedClassCount[0][0] 有了这些函数，我们就可以开始编写构造决策树的函数了：1234567891011121314151617def createTree(dataSet,labels): #构造决策树 classList = [example[-1] for example in dataSet] if classList.count(classList[0]) == len(classList): return classList[0]#若只有一个类别了 if len(dataSet[0]) == 1: return majorityCnt(classList)#若没有特征了，选择主要类别 bestFeat = chooseBestFeatureToSplit(dataSet) bestFeatLabel = labels[bestFeat] myTree = &#123;bestFeatLabel:&#123;&#125;&#125; del labels[bestFeat] featValues = [example[bestFeat] for example in dataSet] uniqueVals = set(featValues) for value in uniqueVals: subLabels = labels[:] myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet,bestFeat,value),subLabels) return myTree 代码使用两个输入参数：数据集和标签列表。标签列表包含了数据集中所有特征的标签，算法本身并不需要这个变量，但是为了给出数据明确的含义，我们将它作为一个输入参数提供。递归函数的第一个停止条件是所有的类标签完全相同，则直接返回该类标签。递归函数的第二个停止条件是使用完了所有的特征，仍然不能将数据集划分成仅包含唯一类别的分组。我们将使用前面的投票表决函数选出出现次数最多的类别作为返回值。这里我们使用字典来存储树的信息，当然也可以使用特殊的数据类型存储树，但是这里没有必要。用字典有利于后面绘制树形图。然后我们试着构建一颗决策树：123myDat,labels = createDataSet()myTree = createTree(myDat,labels)print (myTree) 可以看到结果是：{‘no surfacing’: {0: ‘no’, 1: {‘flippers’: {0: ‘no’, 1: ‘yes’}}}} 3.2 在Python中使用Matplotlib注解绘制树形图上节我们已经学习了如何从数据集中创建树，然而字典的表示形式非常不易于理解，而且直接绘制图形也比较困难。本节我们将使用Matplotlib库创建树形图。决策树的主要优点就是直观易于理解，如果不能将其直观地显示出来，就无法发挥其优势。由于Python没有绘制树的工具，因此我们必须自己绘制树形图。 3.2.1 Matplotlib注解Matplotlib提供了一个非常有用的注解工具annotations，本书将使用其绘制树形图，它可以对文字着色并提供多种形状以供选择。创建treePlotter.py，输入以下代码：1234567891011121314151617181920import matplotlib.pyplot as plt#定义文本框和箭头的格式decisionNode = dict(boxstyle=\"sawtooth\", fc=\"0.8\")#决策节点(非叶节点)leafNode = dict(boxstyle=\"round4\", fc=\"0.8\")#叶节点arrow_args = dict(arrowstyle=\"&lt;-\")#注解def plotNode(nodeTxt,centerPt,parentPt,nodeType): #画出节点和箭头 createPlot.ax1.annotate(nodeTxt,xy=parentPt,xycoords='axes fraction',xytext=centerPt, textcoords='axes fraction',va=\"center\",ha=\"center\", bbox=nodeType,arrowprops=arrow_args)def createPlot(): fig = plt.figure(1,facecolor='white') fig.clf() createPlot.ax1 = plt.subplot(111,frameon=False) plotNode('决策节点',(0.5,0.1),(0.1,0.5),decisionNode) plotNode('叶节点',(0.8,0.1),(0.3,0.8),leafNode) plt.show() 代码先定义了树节点格式，然后用plotNode()函数执行了实际的绘图功能，该函数需要一个绘图区，该区域由全局变量createPlot.ax1提供。然后我们对代码进行测试，输入：1createPlot() 可以得到： 3.2.2 构造注解树绘制一课完整的树需要一些技巧，我们需要知道树的宽度（叶子结点个数）和高度（层数），从可以确定x轴和y轴的长度。12345678910111213141516171819202122def getNumLeafs(myTree): #递归获得该树叶节点的数量 numLeafs = 0 firstStr = list(myTree.keys())[0] secondDict = myTree[firstStr] for key in secondDict.keys(): if type(secondDict[key]).__name__ == 'dict': numLeafs += getNumLeafs(secondDict[key]) else: numLeafs += 1 return numLeafsdef getTreeDepth(myTree): #递归获得树的深度 maxDepth = 0 firstStr = list(myTree.keys())[0] secondDict = myTree[firstStr] for key in secondDict.keys(): if type(secondDict[key]).__name__ == 'dict': thisDepth = 1 + getTreeDepth(secondDict[key]) else: thisDepth = 1 if thisDepth &gt; maxDepth: maxDepth = thisDepth return maxDepth 熟悉递归的话，上面的代码想必不用多说，特别需要注意的是.__name__指的是数据类型名称，用法如代码所示，用于判断当前节点是否已经到最后一层。我们可以编写retrieveTree输出预先存储的树信息，避免每次测试代码都要从数据中创建树的麻烦。123456def retrieveTree(i): #测试用的树 listOfTrees =[&#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125;, &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: &#123;'head': &#123;0: 'no', 1: 'yes'&#125;&#125;, 1: 'no'&#125;&#125;&#125;&#125; ] return listOfTrees[i] 然后输入:1234myTree = retrieveTree(0)print (myTree)print (getNumLeafs(myTree))print (getTreeDepth(myTree)) 然后可以得到：{‘no surfacing’: {0: ‘no’, 1: {‘flippers’: {0: ‘no’, 1: ‘yes’}}}}32现在我们可以将前面学到的方法组合在一起，绘制一颗完整的树。最终的结果如下图所示。但是不需要x和y标签。123456789101112131415161718192021222324252627282930313233343536def plotMidText(cntrPt,parentPt,txtString): #绘制父子节点之间的信息，也就是边的权值，或者说含义 xMid = (parentPt[0] - cntrPt[0])/2.0 + cntrPt[0] yMid = (parentPt[1] - cntrPt[1])/2.0 + cntrPt[1] createPlot.ax1.text(xMid,yMid,txtString)def plotTree(myTree, parentPt, nodeTxt): #绘制决策树 numLeafs = getNumLeafs(myTree) #树的宽度 depth = getTreeDepth(myTree) #树的深度 firstStr = list(myTree.keys())[0] #下一层的节点名称 cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff) plotMidText(cntrPt, parentPt, nodeTxt) plotNode(firstStr, cntrPt, parentPt, decisionNode) secondDict = myTree[firstStr] plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD for key in secondDict.keys(): if type(secondDict[key]).__name__=='dict':#如果下一层不是叶节点 plotTree(secondDict[key],cntrPt,str(key)) #递归子树 else: #如果下一层是叶节点，绘制图形 plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode) plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key)) plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalDdef createPlot(inTree): #设置必要的参数用以绘制决策树 fig = plt.figure(1, facecolor='white') fig.clf() axprops = dict(xticks=[], yticks=[]) createPlot.ax1 = plt.subplot(111, frameon=False, **axprops) plotTree.totalW = float(getNumLeafs(inTree)) #作为全局变量存储树的宽度，即叶节点的个数 plotTree.totalD = float(getTreeDepth(inTree)) #作为全局变量存储树的高度 plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0; plotTree(inTree, (0.5,1.0), '') plt.show() 这段代码比如复杂，主要涉及到计算图形的坐标位置。接下来我们可以输入命令用于测试：12myTree = retrieveTree(1)createPlot(myTree) 于是可以得到： 3.3 测试和存储分类器 3.3.1 测试算法：使用决策树执行分类为了验证算法的实际效果，输入以下代码:1234567891011def classify(inputTree,featLabels,testVec): #用决策树进行分类算法，时间复杂度O(n),n是特征值数量 firstStr = list(inputTree.keys())[0]#树上第一个特征，也就是影响最大的特征 secondDict = inputTree[firstStr] featIndex = featLabels.index(firstStr) for key in secondDict.keys(): if testVec[featIndex] == key: if type(secondDict[key]).__name__ == 'dict': classLabel = classify(secondDict[key],featLabels,testVec) else: classLabel = secondDict[key] return classLabel 该函数也是一个递归函数，我们可以用index方法查找当前列表中第一个匹配firstStr变量的元素下标。接下来进行测试：12345678myDat,labels = trees.createDataSet()print (labels)myTree = retrieveTree(0)print (myTree)createPlot(myTree)ans1 = classify(myTree,labels,[1,0])ans2 = classify(myTree,labels,[1,1])print (ans1,ans2) 可以得到：[‘no surfacing’, ‘flippers’]no yes 对比结果和图片可以发现两个结果符合决策树。 3.3.2 使用算法：决策树的存储构造决策树是很耗时的任务，即使处理很小的数据集，如前面的样本数据，也要花费几秒。所以我们想要把决策树存储下来，在每次执行分类时调用已经构造好的决策树。需要使用Python模块pickle序列化对象，参见以下代码:1234567891011import pickledef storeTree(inputTree,filename): #存储已经构造好的决策树 fw = open(filename,'wb')#pickle只支持二进制存储与读取，所以要加个b pickle.dump(inputTree,fw)#序列化后存在本地 fw.close() def grabTree(filename): #读取已经构造好的决策树 fr = open(filename,'rb') return pickle.load(fr) 输入：123myTree = retrieveTree(0)trees.storeTree(myTree,'a.txt')print (trees.grabTree('a.txt')) 于是可以得到：{‘no surfacing’: {0: ‘no’, 1: {‘flippers’: {0: ‘no’, 1: ‘yes’}}}}可以发现，决策树这种可持久化分类器比k近邻算法要好，下节我们将使用这些工具处理隐形眼镜数据集。 3.4 实例：使用决策树预测隐形眼镜类型首先输入以下命令加载数据：12345fr = open('lenses.txt')lenses = [inst.strip().split('\\t') for inst in fr.readlines()]lensesLabels = ['age','prescript','astigmatic','tearRate']lenseTree = trees.createTree(lenses,lensesLabels)print (lenseTree) 可以看到：{‘tearRate’: {‘normal’: {‘astigmatic’: {‘yes’: {‘prescript’: {‘hyper’: {‘age’: {‘pre’: ‘no lenses’, ‘young’: ‘hard’, ‘presbyopic’: ‘no lenses’}}, ‘myope’: ‘hard’}}, ‘no’: {‘age’: {‘pre’: ‘soft’, ‘young’: ‘soft’, ‘presbyopic’: {‘prescript’: {‘hyper’: ‘soft’, ‘myope’: ‘no lenses’}}}}}}, ‘reduced’: ‘no lenses’}}字典形式很不直观，我们画出图来看看1createPlot(lenseTree) 可以得到:于是我们就可以根据这颗决策树去对未知数据进行分类，但是仔细观察可以发现这些匹配项可能太多了，我们将其称之为过度匹配（过拟合），这也是决策树很突出的一个问题。为了减少过度匹配问题，我们可以裁剪决策树，去掉一些不必要的叶子节点。之后我们将学习另一个决策树构造算法CART，本章所使用的算法称为ID3，但存在太多的数据划分时，ID3算法会面临很多问题。 3.5 本章小结决策树分类器就像带有终止块的流程图，终止块表示分类结果。还有其他的决策树的构造算法，最流行的是C4.5和CART。本书第2章，第3章讨论的是结果确定的分类算法，数据实例最终会被明确划分到某个分类中。下一章我们将讨论朴素贝叶斯，它也是一种分类算法，但是不能完全确定数据实例应该划分到哪个分类，只能给出数据实例属于给定分类的概率。 3.6 本章完整代码 trees.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697from math import logimport pickledef calcShannonEnt(dataSet): #计算数据集的香农熵，表征混乱程度 numEntries = len(dataSet) labelCounts = &#123;&#125; for featVec in dataSet: currentLabel = featVec[-1] if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 labelCounts[currentLabel] += 1 shannonEnt = 0.0 for key in labelCounts: prob = float(labelCounts[key])/numEntries shannonEnt -= prob * log(prob,2) return shannonEntdef createDataSet(): #创建数据集 dataSet = [[1,1,'yes'],[1,1,'yes'],[1,0,'no'],[0,1,'no'],[0,1,'no']] labels = ['no surfacing','flippers'] return dataSet,labelsdef splitDataSet(dataSet,axis,value): #返回第axis个特征为value的的数据集，要去掉第axis个特征 retDataSet = [] for featVec in dataSet: if featVec[axis] == value: reducedFeatVec = featVec[:axis] reducedFeatVec.extend(featVec[axis+1:]) retDataSet.append(reducedFeatVec) return retDataSetdef chooseBestFeatureToSplit(dataSet): #选择当前分类的最佳特征，即信息增益最大的特征 numFeature = len(dataSet[0]) - 1 baseEntropy = calcShannonEnt(dataSet) bestInfoGain = 0.0;bestFeature = -1 for i in range(numFeature):#遍历所有特征 featList = [example[i] for example in dataSet] UniqueFeatList = set(featList) newEntropy = 0.0 for value in UniqueFeatList:#遍历该特征的所有取值，求出平均香农熵 subDataSet = splitDataSet(dataSet,i,value) prob = len(subDataSet) / float(len(dataSet)) newEntropy += prob * calcShannonEnt(subDataSet) infoGain = baseEntropy - newEntropy if infoGain &gt; bestInfoGain:#松弛操作 bestInfoGain = infoGain bestFeature = i return bestFeaturedef majorityCnt(classList): #当所有特征都分类完，但是结果仍不唯一时，选择出现次数最多的类别 classCount = &#123;&#125; for vote in classList: if vote not in classCount.keys(): classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount.items(),key=lambda d:d[1],reversed=True) return sortedClassCount[0][0]def createTree(dataSet,labels): #构造决策树 classList = [example[-1] for example in dataSet] if classList.count(classList[0]) == len(classList): return classList[0]#若只有一个类别了 if len(dataSet[0]) == 1: return majorityCnt(classList)#若没有特征了，选择主要类别 bestFeat = chooseBestFeatureToSplit(dataSet) bestFeatLabel = labels[bestFeat] myTree = &#123;bestFeatLabel:&#123;&#125;&#125; del labels[bestFeat] featValues = [example[bestFeat] for example in dataSet] uniqueVals = set(featValues) for value in uniqueVals: subLabels = labels[:] myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet,bestFeat,value),subLabels) return myTreedef storeTree(inputTree,filename): #存储已经构造好的决策树 fw = open(filename,'wb')#pickle只支持二进制存储与读取，所以要加个b pickle.dump(inputTree,fw)#序列化后存在本地 fw.close() def grabTree(filename): #读取已经构造好的决策树 fr = open(filename,'rb') return pickle.load(fr)#myDat,labels = createDataSet()#myTree = createTree(myDat,labels)#print (myTree) treePlotter.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108import matplotlib.pyplot as pltimport trees#写好的类#定义文本框和箭头的格式decisionNode = dict(boxstyle=\"sawtooth\", fc=\"0.8\")#决策节点(非叶节点)leafNode = dict(boxstyle=\"round4\", fc=\"0.8\")#叶节点arrow_args = dict(arrowstyle=\"&lt;-\")#注解def plotNode(nodeTxt,centerPt,parentPt,nodeType): #画出节点和箭头 createPlot.ax1.annotate(nodeTxt,xy=parentPt,xycoords='axes fraction',xytext=centerPt, textcoords='axes fraction',va=\"center\",ha=\"center\", bbox=nodeType,arrowprops=arrow_args)def getNumLeafs(myTree): #递归获得该树叶节点的数量 numLeafs = 0 firstStr = list(myTree.keys())[0] secondDict = myTree[firstStr] for key in secondDict.keys(): if type(secondDict[key]).__name__ == 'dict': numLeafs += getNumLeafs(secondDict[key]) else: numLeafs += 1 return numLeafsdef getTreeDepth(myTree): #递归获得树的深度 maxDepth = 0 firstStr = list(myTree.keys())[0] secondDict = myTree[firstStr] for key in secondDict.keys(): if type(secondDict[key]).__name__ == 'dict': thisDepth = 1 + getTreeDepth(secondDict[key]) else: thisDepth = 1 if thisDepth &gt; maxDepth: maxDepth = thisDepth return maxDepthdef plotMidText(cntrPt,parentPt,txtString): #绘制父子节点之间的信息，也就是边的权值，或者说含义 xMid = (parentPt[0] - cntrPt[0])/2.0 + cntrPt[0] yMid = (parentPt[1] - cntrPt[1])/2.0 + cntrPt[1] createPlot.ax1.text(xMid,yMid,txtString)def plotTree(myTree, parentPt, nodeTxt): #绘制决策树 numLeafs = getNumLeafs(myTree) #树的宽度 depth = getTreeDepth(myTree) #树的深度 firstStr = list(myTree.keys())[0] #下一层的节点名称 cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff) plotMidText(cntrPt, parentPt, nodeTxt) plotNode(firstStr, cntrPt, parentPt, decisionNode) secondDict = myTree[firstStr] plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD for key in secondDict.keys(): if type(secondDict[key]).__name__=='dict':#如果下一层不是叶节点 plotTree(secondDict[key],cntrPt,str(key)) #递归子树 else: #如果下一层是叶节点，绘制图形 plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode) plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key)) plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalDdef createPlot(inTree): #设置必要的参数用以绘制决策树 fig = plt.figure(1, facecolor='white') fig.clf() axprops = dict(xticks=[], yticks=[]) createPlot.ax1 = plt.subplot(111, frameon=False, **axprops) plotTree.totalW = float(getNumLeafs(inTree)) #作为全局变量存储树的宽度，即叶节点的个数 plotTree.totalD = float(getTreeDepth(inTree)) #作为全局变量存储树的高度 plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0; plotTree(inTree, (0.5,1.0), '') plt.show()def classify(inputTree,featLabels,testVec): #用决策树进行分类算法，时间复杂度O(n),n是特征值数量 firstStr = list(inputTree.keys())[0]#树上第一个特征，也就是影响最大的特征 secondDict = inputTree[firstStr] featIndex = featLabels.index(firstStr) for key in secondDict.keys(): if testVec[featIndex] == key: if type(secondDict[key]).__name__ == 'dict': classLabel = classify(secondDict[key],featLabels,testVec) else: classLabel = secondDict[key] return classLabeldef retrieveTree(i): #测试用的树 listOfTrees =[&#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125;, &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: &#123;'head': &#123;0: 'no', 1: 'yes'&#125;&#125;, 1: 'no'&#125;&#125;&#125;&#125; ] return listOfTrees[i]#def createPlot():# fig = plt.figure(1,facecolor='white')# fig.clf()# createPlot.ax1 = plt.subplot(111,frameon=False)# plotNode('决策节点',(0.5,0.1),(0.1,0.5),decisionNode)# plotNode('叶节点',(0.8,0.1),(0.3,0.8),leafNode)# plt.show()fr = open('lenses.txt')lenses = [inst.strip().split('\\t') for inst in fr.readlines()]lensesLabels = ['age','prescript','astigmatic','tearRate']lenseTree = trees.createTree(lenses,lensesLabels)print (lenseTree)createPlot(lenseTree) 后记更新这样一篇博客太累了（学习第四章去了","tags":[{"name":"机器学习实战","slug":"机器学习实战","permalink":"http://flandre.site/tags/机器学习实战/"},{"name":"机器学习","slug":"机器学习","permalink":"http://flandre.site/tags/机器学习/"}]},{"title":"【机器学习实战】第二章 k近邻算法","date":"2018-05-13T07:37:50.550Z","path":"2018/05/13/【机器学习实战】第二章 k近邻算法/","text":"序言马上就要毕业了，研究生的方向是机器学习和深度学习最近开始学习机器学习，刷完吴恩达的视频后想着应该拿本实战书练手理论+实践才能出真知然后就开始啃这本书，现在看了三章，感觉这本书写的还是相当不错的，简单易懂，能实现一些基本的功能本人的环境是Anaconda3.5，实际运行过程中发现书中不少代码需要小改动，所以如果我的代码和书中不同，也是正常的注意：本博客并非完全照抄书上内容（太多了），只是摘选出本人觉得对学习比较有用的内容+一点个人的理解，并且保证代码是可运行的关于该书的源代码和数据集： 链接：点我 密码：rh3a 这篇博客主要介绍的是这本书的第二章: k近邻算法 2.1 k近邻算法概述 工作原理存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中数据最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k近邻算法中k的出处，通常k&lt;=20。最后选择k个最相似数据中出现次数最多的分类，作为新数据的分类。 k近邻算法 1234567891011121314def classify0(inx,dataSet,labels,k): #knn,inx是输入向量，dataSet和labels是训练集，k是knn的k dataSetSize = dataSet.shape diffMat = np.tile(inx,(dataSetSize[0],1)) - dataSet#np.tile函数是表示把inx沿着行列方向复制几次 sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1)#axis表示求每一行的和 distances = sqDistances ** 0.5 sortDistIndicies = distances.argsort() classCount = &#123;&#125; for i in range(k): label = labels[sortDistIndicies[i]] classCount[label] = classCount.get(label,0) + 1#get方法返回对应的值，若不存在返回默认值0 sortClassCount = sorted(classCount.items(),key=lambda d:d[1],reverse=True)#reverse表示从大到小 return sortClassCount[0][0] classify0用于实现k近邻算法，有4个输入参数：用于分类的输入向量是inX，输入的训练样本集为dataSet，标签向量为labels，最后的参数k表示最近邻居的数目这里关于两个数据之间距离的计算，我们采用欧几里得距离:例如数据集存在4个特征值，则点（7,6,9,4）与（1,0,0,1）之间的距离计算为：计算完所有点之间的距离后，可以对数据按照从小到大的次序排序。然后，确定前k个距离最小元素所在的主要分类，输入k总是正整数；之后将classCount字典分解为元组列表，然后使用lambda内置函数进行排序，注意这里d是一个键值对,d[1]指的就是值,reverse表示此处的排序为逆序，最后返回发生频率最高的元素标签。 2.2 示例: 使用k近邻算法改进约会网站的配对效果背景：我的朋友海伦一直使用在线约会网站寻找适合自己的约会对象，她经过总结发现她曾交往过三种类型的人：（1）不喜欢的人（2）魅力一般的人（3）极具魅力的人海伦希望我们的分类软件可以更好的帮助她将匹配对象划分到确切的分类中 海伦收集约会数据已经有一段时间了，她把这些数据存放在文本文件datingTestSet2.txt中，每个样本数据占据一行，总共有1000行。海伦的样本主要包含以下3种特征：（1）每年获得的飞行常客里程数（2）玩视频游戏所耗时间百分比（3）每周消费的冰淇淋公升数 2.2.1 准备数据:从文本文件中解析数据在将上述特征数据输入到分类器之前，必须将待处理数据的格式改变为分类器可以接受的格式。于是创建file2matrix的函数，以此来处理输入格式问题，该函数输入为文件名字符串，输出为训练样本矩阵和类标签向量。123456789101112131415def file2matrix(filename,n): #读取文件，返回特征矩阵和标签列表 file = open(filename) arrayOLines = file.readlines() numberOfLines = len(arrayOLines) returnMat = np.zeros((numberOfLines,n)) classLabelList = [] index = 0 for line in arrayOLines: line = line.strip() line = line.split('\\t') returnMat[index,:] = line[0:n] classLabelList.append(int(line[-1])) index += 1 return returnMat,classLabelList 2.2.2 分析数据：使用Matplotlib创建散点图 2.2.3 准备数据：归一化数据如果有两组特征数据：（0,20000,1.1）和（67,32000,0.1）那么他们之间的距离为：容易发现，上面方程中数字差值最大的属性对计算结果的影响最大，也即是说，每年获取的飞行常客里程数（此处定为第二个特征）对于计算结果的影响远远大于其他两个特征。但海伦认为这三个特征是同等重要的，因此我们要对数据做归一化处理。说起归一化，我们容易想到将取值范围处理到0~1或者-1~1之间，比如下面这个公式：newValue = (oldValue-min)/(max-min)于是我们可以编写出归一化函数autoNorm的代码：123456789def autoNorm(dataSet): #归一化特征 minVals = dataSet.min(0) maxVals = dataSet.max(0) ranges = maxVals - minVals m = dataSet.shape[0] normDataSet = dataSet - np.tile(minVals,(m,1)) normDataSet = normDataSet/np.tile(ranges,(m,1))#对应位置的值相除，不是矩阵除法 return normDataSet,ranges,minVals 这里解释一下np.tile(A,(n,m))函数,它的意思是把A沿着行复制n遍，沿着列复制m遍比如A=[1,2,3]，n=2，m=2，那么结果就是[1,2,3,1,2,3 1,2,3,1,2,3]，从一个1*3的矩阵变成一个2*6的矩阵 2.2.4 测试算法：作为完整程序验证分类器上节我们已经将数据做了处理，本节我们将测试分类器的效果。通常我们只提供已有数据的90%作为训练样本来训练分类器，而使用其余的10%数据去测试分类器，检测分类器的正确率；所谓的错误率就是分类器给出的错误结果次数除以测试数据的总数，完美分类器的错误率为0，而错误率为1的分类器不会给出任何正确的结果。代码里我们定义一个计数器变量，每次分类器错误地分类数据，计数器就加1，程序完成之后计数器的结果除以数据点总数就是错误率。为了测试分类器效果，创建函数datingClassTest12345678910111213141516def datingClassTest(): #测试分类器的功能 hoRatio = 0.1#测试集占数据集的比例 datingDataMat,datingLabels = file2matrix('datingTestSet2.txt',3) normMat,ranges,minVals = autoNorm(datingDataMat) m = normMat.shape[0] numTestVecs = int(m*hoRatio) errorCount = 0.0 for i in range(numTestVecs): classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:], datingLabels[numTestVecs:m],3) print (\"the classifier came back with: %d, the real answer is: %d\" % (classifierResult,datingLabels[i])) if classifierResult != datingLabels[i]: errorCount += 1.0 print (\"the total error rate is: %f\" % (errorCount/float(numTestVecs))) 注意此处数据是随机的，所以测试集连续也没关系。再输入datingClassTest()调用该函数，我们可以得到：the classifier came back with: 3, the real answer is: 3the classifier came back with: 2, the real answer is: 2the classifier came back with: 1, the real answer is: 1the classifier came back with: 1, the real answer is: 1…the classifier came back with: 2, the real answer is: 2the classifier came back with: 1, the real answer is: 1the classifier came back with: 3, the real answer is: 1the total error rate is: 0.050000分类器处理约会数据集的错误率大概是5%,这是一个不错的结果。我们可以改变hoRatio变量k的值，可以发现结果可能会大不相同。 2.2.5 使用算法：构建完整可用系统我们会给海伦一小段程序，通过改程序海伦会在约会网站上找到某个人并输入他的信息，程序会反馈回她对对方喜欢程度的预测值。于是我们可以编写出约会网站预测函数:1234567891011def classifyPerson(): #预测相亲对象的满意度 resultList = ['not at all','in small doses','in large doses'] percentTats = float(input(\"percentage oftime spent plating video games?\")) ffMiles = float(input(\"frequent flier miles earned per year?\")) iceCream = float(input(\"liters of ice cream consumed per year?\")) datingDataMat,datingLabels = file2matrix('datingTestSet2.txt',3) normMat,ranges,minVals = autoNorm(datingDataMat) inArr = np.array([ffMiles,percentTats,iceCream]) classifierResult = classify0((inArr-minVals)/ranges,normMat,datingLabels,3) print (\"You will probably like this person: \",resultList[classifierResult - 1]) 再输入classifyPerson(),依次输入10,10000,0.5,可以得到如下结果： percentage oftime spent plating video games?10frequent flier miles earned per year?10000liters of ice cream consumed per year?0.5You will probably like this person: in small doses 2.3 手写识别系统 2.3.1 准备数据：将图像转换为测试向量实际图像存储在第二章源代码的两个子目录内：trainingDigits中包含了大约2000个例子，每个例子的内容如下图所示；目录testDigits中包含了大约900个测试数据。我们使用trainingDigits中的数据训练分类器，使用目录testDigits中的数据测试分类器的效果。两组数据没有重叠。为了使用前面两个例子的分类器，我们必须将图像格式化处理为一个向量。我们将把一个32*32的二进制图像矩阵转换为1*1024的向量，这样前两节使用的分类器就可以处理这个数字图像信息了。我们先编写函数img2vector，将图像转换为向量:该函数创建1*1024的Numpy数组，然后打开给定的文件，循环读出文件的前32行，并将每行的头32个字符值存储在Numpy数组中，最后返回数组。12345678def img2vector(filename): returnVcet = np.zeros((1,1024)) fr = open(filename) for i in range(32): line = fr.readline() for j in range(32): returnVcet[0,i*32+j] = int(line[j]) return returnVcet 2.3.2 测试算法：使用k近邻算法识别手写数字上节我们已经将数据处理成分类器可以识别的格式，本节我们将这些数据输入到分类器，检测分类器的执行效果。在写入以下代码前，我们必须确保将from os import listdir写入文件的起始部分，这段代码的主要功能是从os模块中导入函数listir,他可以列出该目录下所有文件的文件名（方便遍历）123456789101112131415161718192021222324def handwritingClassTest(): hwLabels = [] trainingFileList = listdir('digits/trainingDigits')#获取该文件夹下的所有文件名字 m = len(trainingFileList) trainingMat = np.zeros((m,1024)) for i in range(m): fileNameStr = trainingFileList[i] fileStr = fileNameStr.split('.')[0] classNumStr = int(fileStr.split('_')[0]) hwLabels.append(classNumStr) trainingMat[i,:] = img2vector('digits/trainingDigits/%s' % fileNameStr) testFileList = listdir('digits/testDigits') errorCount = 0.0 mTest = len(testFileList) for i in range(mTest): fileNameStr = testFileList[i] fileStr = fileNameStr.split(',')[0] classNumStr = int(fileStr.split('_')[0]) vector = img2vector('digits/testDigits/%s' % fileNameStr) classifierResult = knn.classify0(vector,trainingMat,hwLabels,3) print (\"the classifier came back with: %d, the real answei is: %d\" % (classifierResult,classNumStr)) if classifierResult != classNumStr: errorCount += 1.0 print (\"the total error rate is: %f\" % (errorCount/float(mTest))) 再输入handwritingClassTest(),等待一会可以得到以下结果:the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4…the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the total error rate is: 0.010571 可以看到k近邻算法识别手写数字数据集，错误率仅为1.2%,可以尝试改变k的值、修改函数handwritingClassTest随机选取训练样本、改变训练样本的数目，都会对k近邻算法的错误率产生影响，感兴趣的话可以改变这些变量值，观察错误率的变化。观察以上算法可以发现实际使用时，算法的效果并不高。算法要为每个测试向量做2000次距离计算，每个距离计算包括了1024个维度浮点运算，总计要执行900次，此外还需要为测试向量准备2MB的存储空间。是否存在一种算法可以减少存储空间和计算时间的开销呢？k决策树（kd树）就是k近邻算法的优化版，可以节省大量的计算开销。 2.4 本章小结k近邻算法是分类数据最简单最有效的算法，k近邻算法必须保存全部数据集，如果训练数据集很大，必须使用大量的存储空间。此外，由于必须对数据集中每个数据计算距离值，实际使用时可能非常耗时。k近邻算法的另一个缺陷是它无法给出任何数据的基础结构信息，因此我们也无法知晓平均实例样本具有什么特征。下一章的决策树可以解决这个问题。 2.5 本章完整代码 knn.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import numpy as npimport matplotlib.pyplot as pltimport osplt.rcParams['font.sans-serif'] = ['SimHei']#用来正常显示中文标签plt.rcParams['axes.unicode_minus'] = False #用来正常显示负号def createDataset(): group = np.array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]]) labels = ['A','A','B','B'] return group,labelsdef classify0(inx,dataSet,labels,k): #knn,inx是输入向量，dataSet和labels是训练集，k是knn的k dataSetSize = dataSet.shape diffMat = np.tile(inx,(dataSetSize[0],1)) - dataSet#np.tile函数是表示把inx沿着行列方向复制几次 sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1)#axis表示求每一行的和 distances = sqDistances ** 0.5 sortDistIndicies = distances.argsort() classCount = &#123;&#125; for i in range(k): label = labels[sortDistIndicies[i]] classCount[label] = classCount.get(label,0) + 1#get方法返回对应的值，若不存在返回默认值0 sortClassCount = sorted(classCount.items(),key=lambda d:d[1],reverse=True)#reverse表示从大到小 return sortClassCount[0][0]def file2matrix(filename,n): #读取文件，返回特征矩阵和标签列表 file = open(filename) arrayOLines = file.readlines() numberOfLines = len(arrayOLines) returnMat = np.zeros((numberOfLines,n)) classLabelList = [] index = 0 for line in arrayOLines: line = line.strip() line = line.split('\\t') returnMat[index,:] = line[0:n] classLabelList.append(int(line[-1])) index += 1 return returnMat,classLabelListdef autoNorm(dataSet): #归一化特征 minVals = dataSet.min(0) maxVals = dataSet.max(0) ranges = maxVals - minVals m = dataSet.shape[0] normDataSet = dataSet - np.tile(minVals,(m,1)) normDataSet = normDataSet/np.tile(ranges,(m,1))#对应位置相除，不是矩阵除法 return normDataSet,ranges,minValsdef datingClassTest(): #测试分类器的功能 hoRatio = 0.1#测试集占数据集的比例 datingDataMat,datingLabels = file2matrix('datingTestSet2.txt',3) normMat,ranges,minVals = autoNorm(datingDataMat) m = normMat.shape[0] numTestVecs = int(m*hoRatio) errorCount = 0.0 for i in range(numTestVecs): classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:], datingLabels[numTestVecs:m],3) print (\"the classifier came back with: %d, the real answer is: %d\" % (classifierResult,datingLabels[i])) if classifierResult != datingLabels[i]: errorCount += 1.0 print (\"the total error rate is: %f\" % (errorCount/float(numTestVecs))) def classifyPerson(): #预测相亲对象的满意度 resultList = ['not at all','in small doses','in large doses'] percentTats = float(input(\"percentage oftime spent plating video games?\")) ffMiles = float(input(\"frequent flier miles earned per year?\")) iceCream = float(input(\"liters of ice cream consumed per year?\")) datingDataMat,datingLabels = file2matrix('datingTestSet2.txt',3) normMat,ranges,minVals = autoNorm(datingDataMat) inArr = np.array([ffMiles,percentTats,iceCream]) classifierResult = classify0((inArr-minVals)/ranges,normMat,datingLabels,3) print (\"You will probably like this person: \",resultList[classifierResult - 1]) #classifyPerson()#datingClassTest() find_digit.py(手写数字识别) 123456789101112131415161718192021222324252627282930313233343536373839import knnimport numpy as npfrom os import listdirdef img2vector(filename): returnVcet = np.zeros((1,1024)) fr = open(filename) for i in range(32): line = fr.readline() for j in range(32): returnVcet[0,i*32+j] = int(line[j]) return returnVcetdef handwritingClassTest(): hwLabels = [] trainingFileList = listdir('digits/trainingDigits')#获取该文件夹下的所有文件名字 m = len(trainingFileList) trainingMat = np.zeros((m,1024)) for i in range(m): fileNameStr = trainingFileList[i] fileStr = fileNameStr.split('.')[0] classNumStr = int(fileStr.split('_')[0]) hwLabels.append(classNumStr) trainingMat[i,:] = img2vector('digits/trainingDigits/%s' % fileNameStr) testFileList = listdir('digits/testDigits') errorCount = 0.0 mTest = len(testFileList) for i in range(mTest): fileNameStr = testFileList[i] fileStr = fileNameStr.split(',')[0] classNumStr = int(fileStr.split('_')[0]) vector = img2vector('digits/testDigits/%s' % fileNameStr) classifierResult = knn.classify0(vector,trainingMat,hwLabels,3) print (\"the classifier came back with: %d, the real answei is: %d\" % (classifierResult,classNumStr)) if classifierResult != classNumStr: errorCount += 1.0 print (\"the total error rate is: %f\" % (errorCount/float(mTest)))handwritingClassTest() 后记啊，终于写完了，真累，希望这个系列可以坚持到最后吧（逃感谢观看，最后放张灵梦镇博（笑","tags":[{"name":"机器学习实战","slug":"机器学习实战","permalink":"http://flandre.site/tags/机器学习实战/"},{"name":"机器学习","slug":"机器学习","permalink":"http://flandre.site/tags/机器学习/"}]},{"title":"重新开始更新博客","date":"2018-05-12T12:22:35.078Z","path":"2018/05/12/重新开始更新博客/","text":"很久没有好好更新过博客了，想着是时候好好更新一波了之前的博客由于图片大部分失效，想着也没什么有含金量的内容，就全删了，so，再一次从零开始本人CSDN：https://my.csdn.net/acm_cxq","tags":[{"name":"杂谈","slug":"杂谈","permalink":"http://flandre.site/tags/杂谈/"}]},{"title":"解决markdown无法显示本地图片的办法","date":"2017-12-29T12:16:31.598Z","path":"2017/12/29/解决markdown无法显示本地图片的办法/","text":"如果觉得这个方法麻烦可以试一下QQ空间相册之类的图床，应该还是不用怕被删除的。参考 http://blog.csdn.net/sugar_rainbow/article/details/57415705这里再说一下重点： 把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true 在你的hexo目录下执行这样一句话npm install hexo-asset-image –save 等待一小段时间后，再运行hexo n “xxxx”来生成md博文时，/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹当然直接在_posts文件夹里面创建一个跟你博文同名的文件夹也可以 最后在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片：注意： xxxx是这个md文件的名字，也是同名文件夹的名字。只需要有文件夹名字即可，不需要有什么绝对路径。你想引入的图片就只需要放入xxxx这个文件夹内就好了，很像引用相对路径。","tags":[{"name":"markdown","slug":"markdown","permalink":"http://flandre.site/tags/markdown/"}]}]