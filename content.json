[{"title":"【机器学习实战】第四章 朴素贝叶斯","date":"2018-05-14T12:30:14.432Z","path":"2018/05/14/【机器学习实战】第四章 朴素贝叶斯/","text":"序言本人的环境是Anaconda3.5，实际运行过程中发现书中不少代码需要小改动，所以如果我的代码和书中不同，也是正常的注意：本博客并非完全照抄书上内容（太多了），只是摘选出本人觉得对学习比较有用的内容+一点个人的理解，并且保证代码是可运行的关于该书的源代码和数据集： 链接：点我 密码：rh3a 前面两章我们要求分类器明确给出数据实例应该属于哪一类，不过，分类器有时候会产生错误结果，这时可以要求分类器给出一个最优的类别猜测，并且给出这个猜测的概率估计值。本章主要讲解的就是基于概率论的分类方法——朴素贝叶斯 4.1 基于贝叶斯决策理论的分类方法优点： 在数据较少的情况下仍然有效，可以处理多类别问题。缺点： 对于输入数据的准备方式较为敏感。适用数据类型： 标称型数据 朴素贝叶斯是贝叶斯决策理论的一部分，所以讲述朴素贝叶斯之前有必要快速了解一下贝叶斯决策理论。如果有两个类别，现在有一个数据实例属于类别1的概率是A，属于类别2的概率是B，那么：① 如果A&gt;B，那么该数据实例的类别是1② 如果A&lt;B，那么该数据实例的类别是2也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。 4.2 条件概率这节我们将会稍微讲解一下概率论的基础知识之一条件概率P(A|B)表示在B发生的情况下，A发生的概率，这也是条件概率的定义，并且有P(A|B) = P(AB) / P(B)基于该公式，我们可以得出贝叶斯准则，贝叶斯准则告诉我们如何交换条件概率中的条件与结果，贝叶斯准则可以表示为：","tags":[{"name":"机器学习实战","slug":"机器学习实战","permalink":"http://flandre.site/tags/机器学习实战/"},{"name":"机器学习","slug":"机器学习","permalink":"http://flandre.site/tags/机器学习/"}]},{"title":"【机器学习实战】第三章 决策树","date":"2018-05-13T13:33:53.940Z","path":"2018/05/13/【机器学习实战】第三章 决策树/","text":"序言本人的环境是Anaconda3.5，实际运行过程中发现书中不少代码需要小改动，所以如果我的代码和书中不同，也是正常的注意：本博客并非完全照抄书上内容（太多了），只是摘选出本人觉得对学习比较有用的内容+一点个人的理解，并且保证代码是可运行的关于该书的源代码和数据集： 链接：点我 密码：rh3a 这篇博客主要介绍的是这本书的第三章: 决策树 如果你以前没有接触过决策树，完全不用担心，它的概念非常简单。如下图就是一个决策树，分支节点就是决策用的某一个特征值，叶子结点表示已经得出结果。可想而知，当我们构造出这样一颗决策树，对于一个未知结果的输入向量，能够很快得到预测结果。专家系统中经常使用决策树，而且决策树给出结果往往可以匹敌在当前领域具有几十年工作经验的人类专家。决策树的优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。缺点：可能会产生过度匹配问题使用数据类型：数值型和标称型 3.1 决策树的构造在构造决策树时，我们需要解决的第一个问题就是当前数据集上哪个特征在划分数据分类时起决定性作用，也就是说选择这个特征值可以使决策树更小。为了找到决定性的特征，划分出最好的结果，我们必须评估每一个特征。完成测试之后，原始数据集就被划分成几个数据子集。这些数据子集会分布在第一个决策点的所有分支上。如果某个分支下的数据属于同一类型，则当前已经正确划分数据分类；无需进一步对数据集进行分割。如果数据子集内的数据不属于同一类型，则需要重复划分数据子集。划分数据子集的算法和划分原始数据集的方法相同，直到所有具有相同类型的数据均在一个数据子集内。 3.1.1 信息增益划分数据集的大原则是：将无序的数据变得更加有序。我们可以使用多种方法划分数据集，但是每种方法都有各自的优缺点。组织杂乱无章数据的一种方法就是使用信息论度量信息。在划分数据集之前之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。什么是熵？熵定义为信息的期望值，在明晰这个概念之前，我们必须知道信息的定义。如果待分类的事物可能划分在多个分类之中，则符号xi的信息定义为其中p(xi)是选择分类xi的概率，显然信息量的取值范围在(0,+∞)为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，可以通过下面的公式得到：其中n是分类的数目，从公式可以看出这个信息熵相当于所有分类的期望熵，由此公式我们可以计算给定数据集的熵，代码如下：12345678910111213141516from math import logdef calcShannonEnt(dataSet): #计算数据集的香农熵，表征混乱程度 numEntries = len(dataSet) labelCounts = &#123;&#125;#该数据集所有分类出现的次数 for featVec in dataSet: currentLabel = featVec[-1] if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 labelCounts[currentLabel] += 1 shannonEnt = 0.0 for key in labelCounts: prob = float(labelCounts[key])/numEntries shannonEnt -= prob * log(prob,2) return shannonEnt 为了测试这个函数的功能，我们可以利用createDataSet()函数得到简单的鱼类鉴定数据集12345def createDataSet(): #创建数据集 dataSet = [[1,1,'yes'],[1,1,'yes'],[1,0,'no'],[0,1,'no'],[0,1,'no']] labels = ['no surfacing','flippers']#两个特征的含义 return dataSet,labels 再输入：123myDat,labels = createDataSet()print (myDat)print (calcShannonEnt(myDat)) 我们可以得到如下结果：[[1, 1, ‘yes’], [1, 1, ‘yes’], [1, 0, ‘no’], [0, 1, ‘no’], [0, 1, ‘no’]]0.9709505944546686熵越高，则混合的数据也越多，数据更加无序。得到熵之后，我们就可以按照获取最大信息增益的方法划分数据集，下一节我们将具体学习如何划分数据集以及如何度量信息增益。 3.1.2 划分数据集上节我们学习了如何度量数据集的无序程度。我们将对每个特征划分数据集的结果计算一次信息熵，然后判断按照哪个特征划分数据集是最好的划分方式。123456789def splitDataSet(dataSet,axis,value): #返回第axis个特征为value的的数据集，要去掉第axis个特征 retDataSet = [] for featVec in dataSet: if featVec[axis] == value: reducedFeatVec = featVec[:axis] reducedFeatVec.extend(featVec[axis+1:]) retDataSet.append(reducedFeatVec) return retDataSet 以上代码使用了三个输入参数：待划分的数据集、目标特征的下标，目标特征。注意区分append和extend的区别假如a = [1,2,3],b = [4,5,6]则a.append(b)的结果是[1,2,3,[4,5,6]]a.extend(b)的结果是[1,2,3,4,5,6]我们可以在前面的简单样本数据上测试该函数输入:1234myDat,labels = createDataSet()print (myDat)print (splitDataSet(myDat,0,1))print (splitDataSet(myDat,0,0)) 我们可以得到：[[1, 1, ‘yes’], [1, 1, ‘yes’], [1, 0, ‘no’], [0, 1, ‘no’], [0, 1, ‘no’]][[1, ‘yes’], [1, ‘yes’], [0, ‘no’]][[1, ‘no’], [1, ‘no’]]我们便用第一个特征对原数据集进行了划分，分成了两个子集接下来我们将遍历整个数据集，循环计算香农熵和splitDataSet()函数，找到最好的特征划分方式。123456789101112131415161718def chooseBestFeatureToSplit(dataSet): #选择当前分类的最佳特征，即信息增益最大的特征 numFeature = len(dataSet[0]) - 1 baseEntropy = calcShannonEnt(dataSet) bestInfoGain = 0.0;bestFeature = -1 for i in range(numFeature):#遍历所有特征 featList = [example[i] for example in dataSet] UniqueFeatList = set(featList) newEntropy = 0.0 for value in UniqueFeatList:#遍历该特征的所有取值，求出平均香农熵 subDataSet = splitDataSet(dataSet,i,value) prob = len(subDataSet) / float(len(dataSet)) newEntropy += prob * calcShannonEnt(subDataSet) infoGain = baseEntropy - newEntropy if infoGain &gt; bestInfoGain:#松弛操作 bestInfoGain = infoGain bestFeature = i return bestFeature 以上代码实现选取特征，划分数据集，计算得出最好的划分数据集的特征。在函数中调用的数据需要满足一定的要求，第一个要求是dataSet必须是一种由列表元素组成的列表，而且所有的列表元素都必须具有相同的数据长度；第二个要求是数据的最后一列或者说每个实例的最后一个元素是当前实例的类别标签（也就是结果）。数据集只要满足上述要求，我们就可以在函数的第一行判定当前数据集包含多少特征属性。我们无需指定lsit中的数据类型，它们既可以是数字也可以是字符串，并不影响实际计算。在开始划分数据集之前，第三行代码计算机整个数据集的原始香农熵，我们保存最初的无需度量值，用于与划分完之后的数据集计算的熵值进行比较。set函数用于去重。遍历当前特征中的所有的唯一属性值，对每个唯一属性划分一次数据集，然后计算数据集的新熵值，并对所有唯一特征值得到的熵求和。最后比较所有特征中的信息增益，返回最好特征划分的索引值。接下来我们稍微测试一下，输入下列命令：1234myDat,labels = createDataSet()feat = chooseBestFeatureToSplit(myDat)print (feat)print (myDat) 然后我们可以得到：0[[1, 1, ‘yes’], [1, 1, ‘yes’], [1, 0, ‘no’], [0, 1, ‘no’], [0, 1, ‘no’]]运行结果告诉我们，第0个特征是最好的用于第一次划分数据集的特征。观察可以看出这个结果是符合的。 3.1.3 递归构建决策树目前我们已经学习了从数据集构造决策树算法所需要的子功能模块，其工作原理如下：得到原始数据集，然后基于最好的特征划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。因此我们可以才用递归处理数据集。递归结束的条件是：程序遍历完所有划分数据集的特征，或者每个分支下的所有实例都具有相同的分类。如果所有实例具有相同的分类，则得到一个叶子结点或者终止块。任何到达叶子节点的数据必须属于叶子结点所属的分类。如果数据集已经处理了所有的特征，但是类标签仍然不是唯一的，此时我们需要决定如何定义该叶子结点，在这种情况下，我们通常会采用多数表决的方法决定该叶子节点的分类。123456789def majorityCnt(classList): #当所有特征都分类完，但是结果仍不唯一时，选择出现次数最多的类别 classCount = &#123;&#125; for vote in classList: if vote not in classCount.keys(): classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount.items(),key=lambda d:d[1],reversed=True) return sortedClassCount[0][0] 有了这些函数，我们就可以开始编写构造决策树的函数了：1234567891011121314151617def createTree(dataSet,labels): #构造决策树 classList = [example[-1] for example in dataSet] if classList.count(classList[0]) == len(classList): return classList[0]#若只有一个类别了 if len(dataSet[0]) == 1: return majorityCnt(classList)#若没有特征了，选择主要类别 bestFeat = chooseBestFeatureToSplit(dataSet) bestFeatLabel = labels[bestFeat] myTree = &#123;bestFeatLabel:&#123;&#125;&#125; del labels[bestFeat] featValues = [example[bestFeat] for example in dataSet] uniqueVals = set(featValues) for value in uniqueVals: subLabels = labels[:] myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet,bestFeat,value),subLabels) return myTree 代码使用两个输入参数：数据集和标签列表。标签列表包含了数据集中所有特征的标签，算法本身并不需要这个变量，但是为了给出数据明确的含义，我们将它作为一个输入参数提供。递归函数的第一个停止条件是所有的类标签完全相同，则直接返回该类标签。递归函数的第二个停止条件是使用完了所有的特征，仍然不能将数据集划分成仅包含唯一类别的分组。我们将使用前面的投票表决函数选出出现次数最多的类别作为返回值。这里我们使用字典来存储树的信息，当然也可以使用特殊的数据类型存储树，但是这里没有必要。用字典有利于后面绘制树形图。然后我们试着构建一颗决策树：123myDat,labels = createDataSet()myTree = createTree(myDat,labels)print (myTree) 可以看到结果是：{‘no surfacing’: {0: ‘no’, 1: {‘flippers’: {0: ‘no’, 1: ‘yes’}}}} 3.2 在Python中使用Matplotlib注解绘制树形图上节我们已经学习了如何从数据集中创建树，然而字典的表示形式非常不易于理解，而且直接绘制图形也比较困难。本节我们将使用Matplotlib库创建树形图。决策树的主要优点就是直观易于理解，如果不能将其直观地显示出来，就无法发挥其优势。由于Python没有绘制树的工具，因此我们必须自己绘制树形图。 3.2.1 Matplotlib注解Matplotlib提供了一个非常有用的注解工具annotations，本书将使用其绘制树形图，它可以对文字着色并提供多种形状以供选择。创建treePlotter.py，输入以下代码：1234567891011121314151617181920import matplotlib.pyplot as plt#定义文本框和箭头的格式decisionNode = dict(boxstyle=\"sawtooth\", fc=\"0.8\")#决策节点(非叶节点)leafNode = dict(boxstyle=\"round4\", fc=\"0.8\")#叶节点arrow_args = dict(arrowstyle=\"&lt;-\")#注解def plotNode(nodeTxt,centerPt,parentPt,nodeType): #画出节点和箭头 createPlot.ax1.annotate(nodeTxt,xy=parentPt,xycoords='axes fraction',xytext=centerPt, textcoords='axes fraction',va=\"center\",ha=\"center\", bbox=nodeType,arrowprops=arrow_args)def createPlot(): fig = plt.figure(1,facecolor='white') fig.clf() createPlot.ax1 = plt.subplot(111,frameon=False) plotNode('决策节点',(0.5,0.1),(0.1,0.5),decisionNode) plotNode('叶节点',(0.8,0.1),(0.3,0.8),leafNode) plt.show() 代码先定义了树节点格式，然后用plotNode()函数执行了实际的绘图功能，该函数需要一个绘图区，该区域由全局变量createPlot.ax1提供。然后我们对代码进行测试，输入：1createPlot() 可以得到： 3.2.2 构造注解树绘制一课完整的树需要一些技巧，我们需要知道树的宽度（叶子结点个数）和高度（层数），从可以确定x轴和y轴的长度。12345678910111213141516171819202122def getNumLeafs(myTree): #递归获得该树叶节点的数量 numLeafs = 0 firstStr = list(myTree.keys())[0] secondDict = myTree[firstStr] for key in secondDict.keys(): if type(secondDict[key]).__name__ == 'dict': numLeafs += getNumLeafs(secondDict[key]) else: numLeafs += 1 return numLeafsdef getTreeDepth(myTree): #递归获得树的深度 maxDepth = 0 firstStr = list(myTree.keys())[0] secondDict = myTree[firstStr] for key in secondDict.keys(): if type(secondDict[key]).__name__ == 'dict': thisDepth = 1 + getTreeDepth(secondDict[key]) else: thisDepth = 1 if thisDepth &gt; maxDepth: maxDepth = thisDepth return maxDepth 熟悉递归的话，上面的代码想必不用多说，特别需要注意的是.__name__指的是数据类型名称，用法如代码所示，用于判断当前节点是否已经到最后一层。我们可以编写retrieveTree输出预先存储的树信息，避免每次测试代码都要从数据中创建树的麻烦。123456def retrieveTree(i): #测试用的树 listOfTrees =[&#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125;, &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: &#123;'head': &#123;0: 'no', 1: 'yes'&#125;&#125;, 1: 'no'&#125;&#125;&#125;&#125; ] return listOfTrees[i] 然后输入:1234myTree = retrieveTree(0)print (myTree)print (getNumLeafs(myTree))print (getTreeDepth(myTree)) 然后可以得到：{‘no surfacing’: {0: ‘no’, 1: {‘flippers’: {0: ‘no’, 1: ‘yes’}}}}32现在我们可以将前面学到的方法组合在一起，绘制一颗完整的树。最终的结果如下图所示。但是不需要x和y标签。123456789101112131415161718192021222324252627282930313233343536def plotMidText(cntrPt,parentPt,txtString): #绘制父子节点之间的信息，也就是边的权值，或者说含义 xMid = (parentPt[0] - cntrPt[0])/2.0 + cntrPt[0] yMid = (parentPt[1] - cntrPt[1])/2.0 + cntrPt[1] createPlot.ax1.text(xMid,yMid,txtString)def plotTree(myTree, parentPt, nodeTxt): #绘制决策树 numLeafs = getNumLeafs(myTree) #树的宽度 depth = getTreeDepth(myTree) #树的深度 firstStr = list(myTree.keys())[0] #下一层的节点名称 cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff) plotMidText(cntrPt, parentPt, nodeTxt) plotNode(firstStr, cntrPt, parentPt, decisionNode) secondDict = myTree[firstStr] plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD for key in secondDict.keys(): if type(secondDict[key]).__name__=='dict':#如果下一层不是叶节点 plotTree(secondDict[key],cntrPt,str(key)) #递归子树 else: #如果下一层是叶节点，绘制图形 plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode) plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key)) plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalDdef createPlot(inTree): #设置必要的参数用以绘制决策树 fig = plt.figure(1, facecolor='white') fig.clf() axprops = dict(xticks=[], yticks=[]) createPlot.ax1 = plt.subplot(111, frameon=False, **axprops) plotTree.totalW = float(getNumLeafs(inTree)) #作为全局变量存储树的宽度，即叶节点的个数 plotTree.totalD = float(getTreeDepth(inTree)) #作为全局变量存储树的高度 plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0; plotTree(inTree, (0.5,1.0), '') plt.show() 这段代码比如复杂，主要涉及到计算图形的坐标位置。接下来我们可以输入命令用于测试：12myTree = retrieveTree(1)createPlot(myTree) 于是可以得到： 3.3 测试和存储分类器 3.3.1 测试算法：使用决策树执行分类为了验证算法的实际效果，输入以下代码:1234567891011def classify(inputTree,featLabels,testVec): #用决策树进行分类算法，时间复杂度O(n),n是特征值数量 firstStr = list(inputTree.keys())[0]#树上第一个特征，也就是影响最大的特征 secondDict = inputTree[firstStr] featIndex = featLabels.index(firstStr) for key in secondDict.keys(): if testVec[featIndex] == key: if type(secondDict[key]).__name__ == 'dict': classLabel = classify(secondDict[key],featLabels,testVec) else: classLabel = secondDict[key] return classLabel 该函数也是一个递归函数，我们可以用index方法查找当前列表中第一个匹配firstStr变量的元素下标。接下来进行测试：12345678myDat,labels = trees.createDataSet()print (labels)myTree = retrieveTree(0)print (myTree)createPlot(myTree)ans1 = classify(myTree,labels,[1,0])ans2 = classify(myTree,labels,[1,1])print (ans1,ans2) 可以得到：[‘no surfacing’, ‘flippers’]no yes 对比结果和图片可以发现两个结果符合决策树。 3.3.2 使用算法：决策树的存储构造决策树是很耗时的任务，即使处理很小的数据集，如前面的样本数据，也要花费几秒。所以我们想要把决策树存储下来，在每次执行分类时调用已经构造好的决策树。需要使用Python模块pickle序列化对象，参见以下代码:1234567891011import pickledef storeTree(inputTree,filename): #存储已经构造好的决策树 fw = open(filename,'wb')#pickle只支持二进制存储与读取，所以要加个b pickle.dump(inputTree,fw)#序列化后存在本地 fw.close() def grabTree(filename): #读取已经构造好的决策树 fr = open(filename,'rb') return pickle.load(fr) 输入：123myTree = retrieveTree(0)trees.storeTree(myTree,'a.txt')print (trees.grabTree('a.txt')) 于是可以得到：{‘no surfacing’: {0: ‘no’, 1: {‘flippers’: {0: ‘no’, 1: ‘yes’}}}}可以发现，决策树这种可持久化分类器比k近邻算法要好，下节我们将使用这些工具处理隐形眼镜数据集。 3.4 实例：使用决策树预测隐形眼镜类型首先输入以下命令加载数据：12345fr = open('lenses.txt')lenses = [inst.strip().split('\\t') for inst in fr.readlines()]lensesLabels = ['age','prescript','astigmatic','tearRate']lenseTree = trees.createTree(lenses,lensesLabels)print (lenseTree) 可以看到：{‘tearRate’: {‘normal’: {‘astigmatic’: {‘yes’: {‘prescript’: {‘hyper’: {‘age’: {‘pre’: ‘no lenses’, ‘young’: ‘hard’, ‘presbyopic’: ‘no lenses’}}, ‘myope’: ‘hard’}}, ‘no’: {‘age’: {‘pre’: ‘soft’, ‘young’: ‘soft’, ‘presbyopic’: {‘prescript’: {‘hyper’: ‘soft’, ‘myope’: ‘no lenses’}}}}}}, ‘reduced’: ‘no lenses’}}字典形式很不直观，我们画出图来看看1createPlot(lenseTree) 可以得到:于是我们就可以根据这颗决策树去对未知数据进行分类，但是仔细观察可以发现这些匹配项可能太多了，我们将其称之为过度匹配（过拟合），这也是决策树很突出的一个问题。为了减少过度匹配问题，我们可以裁剪决策树，去掉一些不必要的叶子节点。之后我们将学习另一个决策树构造算法CART，本章所使用的算法称为ID3，但存在太多的数据划分时，ID3算法会面临很多问题。 3.5 本章小结决策树分类器就像带有终止块的流程图，终止块表示分类结果。还有其他的决策树的构造算法，最流行的是C4.5和CART。本书第2章，第3章讨论的是结果确定的分类算法，数据实例最终会被明确划分到某个分类中。下一章我们将讨论朴素贝叶斯，它也是一种分类算法，但是不能完全确定数据实例应该划分到哪个分类，只能给出数据实例属于给定分类的概率。 3.6 本章完整代码 trees.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697from math import logimport pickledef calcShannonEnt(dataSet): #计算数据集的香农熵，表征混乱程度 numEntries = len(dataSet) labelCounts = &#123;&#125; for featVec in dataSet: currentLabel = featVec[-1] if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 labelCounts[currentLabel] += 1 shannonEnt = 0.0 for key in labelCounts: prob = float(labelCounts[key])/numEntries shannonEnt -= prob * log(prob,2) return shannonEntdef createDataSet(): #创建数据集 dataSet = [[1,1,'yes'],[1,1,'yes'],[1,0,'no'],[0,1,'no'],[0,1,'no']] labels = ['no surfacing','flippers'] return dataSet,labelsdef splitDataSet(dataSet,axis,value): #返回第axis个特征为value的的数据集，要去掉第axis个特征 retDataSet = [] for featVec in dataSet: if featVec[axis] == value: reducedFeatVec = featVec[:axis] reducedFeatVec.extend(featVec[axis+1:]) retDataSet.append(reducedFeatVec) return retDataSetdef chooseBestFeatureToSplit(dataSet): #选择当前分类的最佳特征，即信息增益最大的特征 numFeature = len(dataSet[0]) - 1 baseEntropy = calcShannonEnt(dataSet) bestInfoGain = 0.0;bestFeature = -1 for i in range(numFeature):#遍历所有特征 featList = [example[i] for example in dataSet] UniqueFeatList = set(featList) newEntropy = 0.0 for value in UniqueFeatList:#遍历该特征的所有取值，求出平均香农熵 subDataSet = splitDataSet(dataSet,i,value) prob = len(subDataSet) / float(len(dataSet)) newEntropy += prob * calcShannonEnt(subDataSet) infoGain = baseEntropy - newEntropy if infoGain &gt; bestInfoGain:#松弛操作 bestInfoGain = infoGain bestFeature = i return bestFeaturedef majorityCnt(classList): #当所有特征都分类完，但是结果仍不唯一时，选择出现次数最多的类别 classCount = &#123;&#125; for vote in classList: if vote not in classCount.keys(): classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount.items(),key=lambda d:d[1],reversed=True) return sortedClassCount[0][0]def createTree(dataSet,labels): #构造决策树 classList = [example[-1] for example in dataSet] if classList.count(classList[0]) == len(classList): return classList[0]#若只有一个类别了 if len(dataSet[0]) == 1: return majorityCnt(classList)#若没有特征了，选择主要类别 bestFeat = chooseBestFeatureToSplit(dataSet) bestFeatLabel = labels[bestFeat] myTree = &#123;bestFeatLabel:&#123;&#125;&#125; del labels[bestFeat] featValues = [example[bestFeat] for example in dataSet] uniqueVals = set(featValues) for value in uniqueVals: subLabels = labels[:] myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet,bestFeat,value),subLabels) return myTreedef storeTree(inputTree,filename): #存储已经构造好的决策树 fw = open(filename,'wb')#pickle只支持二进制存储与读取，所以要加个b pickle.dump(inputTree,fw)#序列化后存在本地 fw.close() def grabTree(filename): #读取已经构造好的决策树 fr = open(filename,'rb') return pickle.load(fr)#myDat,labels = createDataSet()#myTree = createTree(myDat,labels)#print (myTree) treePlotter.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108import matplotlib.pyplot as pltimport trees#写好的类#定义文本框和箭头的格式decisionNode = dict(boxstyle=\"sawtooth\", fc=\"0.8\")#决策节点(非叶节点)leafNode = dict(boxstyle=\"round4\", fc=\"0.8\")#叶节点arrow_args = dict(arrowstyle=\"&lt;-\")#注解def plotNode(nodeTxt,centerPt,parentPt,nodeType): #画出节点和箭头 createPlot.ax1.annotate(nodeTxt,xy=parentPt,xycoords='axes fraction',xytext=centerPt, textcoords='axes fraction',va=\"center\",ha=\"center\", bbox=nodeType,arrowprops=arrow_args)def getNumLeafs(myTree): #递归获得该树叶节点的数量 numLeafs = 0 firstStr = list(myTree.keys())[0] secondDict = myTree[firstStr] for key in secondDict.keys(): if type(secondDict[key]).__name__ == 'dict': numLeafs += getNumLeafs(secondDict[key]) else: numLeafs += 1 return numLeafsdef getTreeDepth(myTree): #递归获得树的深度 maxDepth = 0 firstStr = list(myTree.keys())[0] secondDict = myTree[firstStr] for key in secondDict.keys(): if type(secondDict[key]).__name__ == 'dict': thisDepth = 1 + getTreeDepth(secondDict[key]) else: thisDepth = 1 if thisDepth &gt; maxDepth: maxDepth = thisDepth return maxDepthdef plotMidText(cntrPt,parentPt,txtString): #绘制父子节点之间的信息，也就是边的权值，或者说含义 xMid = (parentPt[0] - cntrPt[0])/2.0 + cntrPt[0] yMid = (parentPt[1] - cntrPt[1])/2.0 + cntrPt[1] createPlot.ax1.text(xMid,yMid,txtString)def plotTree(myTree, parentPt, nodeTxt): #绘制决策树 numLeafs = getNumLeafs(myTree) #树的宽度 depth = getTreeDepth(myTree) #树的深度 firstStr = list(myTree.keys())[0] #下一层的节点名称 cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff) plotMidText(cntrPt, parentPt, nodeTxt) plotNode(firstStr, cntrPt, parentPt, decisionNode) secondDict = myTree[firstStr] plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD for key in secondDict.keys(): if type(secondDict[key]).__name__=='dict':#如果下一层不是叶节点 plotTree(secondDict[key],cntrPt,str(key)) #递归子树 else: #如果下一层是叶节点，绘制图形 plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode) plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key)) plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalDdef createPlot(inTree): #设置必要的参数用以绘制决策树 fig = plt.figure(1, facecolor='white') fig.clf() axprops = dict(xticks=[], yticks=[]) createPlot.ax1 = plt.subplot(111, frameon=False, **axprops) plotTree.totalW = float(getNumLeafs(inTree)) #作为全局变量存储树的宽度，即叶节点的个数 plotTree.totalD = float(getTreeDepth(inTree)) #作为全局变量存储树的高度 plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0; plotTree(inTree, (0.5,1.0), '') plt.show()def classify(inputTree,featLabels,testVec): #用决策树进行分类算法，时间复杂度O(n),n是特征值数量 firstStr = list(inputTree.keys())[0]#树上第一个特征，也就是影响最大的特征 secondDict = inputTree[firstStr] featIndex = featLabels.index(firstStr) for key in secondDict.keys(): if testVec[featIndex] == key: if type(secondDict[key]).__name__ == 'dict': classLabel = classify(secondDict[key],featLabels,testVec) else: classLabel = secondDict[key] return classLabeldef retrieveTree(i): #测试用的树 listOfTrees =[&#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125;, &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: &#123;'head': &#123;0: 'no', 1: 'yes'&#125;&#125;, 1: 'no'&#125;&#125;&#125;&#125; ] return listOfTrees[i]#def createPlot():# fig = plt.figure(1,facecolor='white')# fig.clf()# createPlot.ax1 = plt.subplot(111,frameon=False)# plotNode('决策节点',(0.5,0.1),(0.1,0.5),decisionNode)# plotNode('叶节点',(0.8,0.1),(0.3,0.8),leafNode)# plt.show()fr = open('lenses.txt')lenses = [inst.strip().split('\\t') for inst in fr.readlines()]lensesLabels = ['age','prescript','astigmatic','tearRate']lenseTree = trees.createTree(lenses,lensesLabels)print (lenseTree)createPlot(lenseTree) 后记更新这样一篇博客太累了（学习第四章去了","tags":[{"name":"机器学习实战","slug":"机器学习实战","permalink":"http://flandre.site/tags/机器学习实战/"},{"name":"机器学习","slug":"机器学习","permalink":"http://flandre.site/tags/机器学习/"}]},{"title":"【机器学习实战】第二章 k近邻算法","date":"2018-05-13T07:37:50.550Z","path":"2018/05/13/【机器学习实战】第二章 k近邻算法/","text":"序言马上就要毕业了，研究生的方向是机器学习和深度学习最近开始学习机器学习，刷完吴恩达的视频后想着应该拿本实战书练手理论+实践才能出真知然后就开始啃这本书，现在看了三章，感觉这本书写的还是相当不错的，简单易懂，能实现一些基本的功能本人的环境是Anaconda3.5，实际运行过程中发现书中不少代码需要小改动，所以如果我的代码和书中不同，也是正常的注意：本博客并非完全照抄书上内容（太多了），只是摘选出本人觉得对学习比较有用的内容+一点个人的理解，并且保证代码是可运行的关于该书的源代码和数据集： 链接：点我 密码：rh3a 这篇博客主要介绍的是这本书的第二章: k近邻算法 2.1 k近邻算法概述 工作原理存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中数据最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k近邻算法中k的出处，通常k&lt;=20。最后选择k个最相似数据中出现次数最多的分类，作为新数据的分类。 k近邻算法 1234567891011121314def classify0(inx,dataSet,labels,k): #knn,inx是输入向量，dataSet和labels是训练集，k是knn的k dataSetSize = dataSet.shape diffMat = np.tile(inx,(dataSetSize[0],1)) - dataSet#np.tile函数是表示把inx沿着行列方向复制几次 sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1)#axis表示求每一行的和 distances = sqDistances ** 0.5 sortDistIndicies = distances.argsort() classCount = &#123;&#125; for i in range(k): label = labels[sortDistIndicies[i]] classCount[label] = classCount.get(label,0) + 1#get方法返回对应的值，若不存在返回默认值0 sortClassCount = sorted(classCount.items(),key=lambda d:d[1],reverse=True)#reverse表示从大到小 return sortClassCount[0][0] classify0用于实现k近邻算法，有4个输入参数：用于分类的输入向量是inX，输入的训练样本集为dataSet，标签向量为labels，最后的参数k表示最近邻居的数目这里关于两个数据之间距离的计算，我们采用欧几里得距离:例如数据集存在4个特征值，则点（7,6,9,4）与（1,0,0,1）之间的距离计算为：计算完所有点之间的距离后，可以对数据按照从小到大的次序排序。然后，确定前k个距离最小元素所在的主要分类，输入k总是正整数；之后将classCount字典分解为元组列表，然后使用lambda内置函数进行排序，注意这里d是一个键值对,d[1]指的就是值,reverse表示此处的排序为逆序，最后返回发生频率最高的元素标签。 2.2 示例: 使用k近邻算法改进约会网站的配对效果背景：我的朋友海伦一直使用在线约会网站寻找适合自己的约会对象，她经过总结发现她曾交往过三种类型的人：（1）不喜欢的人（2）魅力一般的人（3）极具魅力的人海伦希望我们的分类软件可以更好的帮助她将匹配对象划分到确切的分类中 海伦收集约会数据已经有一段时间了，她把这些数据存放在文本文件datingTestSet2.txt中，每个样本数据占据一行，总共有1000行。海伦的样本主要包含以下3种特征：（1）每年获得的飞行常客里程数（2）玩视频游戏所耗时间百分比（3）每周消费的冰淇淋公升数 2.2.1 准备数据:从文本文件中解析数据在将上述特征数据输入到分类器之前，必须将待处理数据的格式改变为分类器可以接受的格式。于是创建file2matrix的函数，以此来处理输入格式问题，该函数输入为文件名字符串，输出为训练样本矩阵和类标签向量。123456789101112131415def file2matrix(filename,n): #读取文件，返回特征矩阵和标签列表 file = open(filename) arrayOLines = file.readlines() numberOfLines = len(arrayOLines) returnMat = np.zeros((numberOfLines,n)) classLabelList = [] index = 0 for line in arrayOLines: line = line.strip() line = line.split('\\t') returnMat[index,:] = line[0:n] classLabelList.append(int(line[-1])) index += 1 return returnMat,classLabelList 2.2.2 分析数据：使用Matplotlib创建散点图 2.2.3 准备数据：归一化数据如果有两组特征数据：（0,20000,1.1）和（67,32000,0.1）那么他们之间的距离为：容易发现，上面方程中数字差值最大的属性对计算结果的影响最大，也即是说，每年获取的飞行常客里程数（此处定为第二个特征）对于计算结果的影响远远大于其他两个特征。但海伦认为这三个特征是同等重要的，因此我们要对数据做归一化处理。说起归一化，我们容易想到将取值范围处理到0~1或者-1~1之间，比如下面这个公式：newValue = (oldValue-min)/(max-min)于是我们可以编写出归一化函数autoNorm的代码：123456789def autoNorm(dataSet): #归一化特征 minVals = dataSet.min(0) maxVals = dataSet.max(0) ranges = maxVals - minVals m = dataSet.shape[0] normDataSet = dataSet - np.tile(minVals,(m,1)) normDataSet = normDataSet/np.tile(ranges,(m,1))#对应位置的值相除，不是矩阵除法 return normDataSet,ranges,minVals 这里解释一下np.tile(A,(n,m))函数,它的意思是把A沿着行复制n遍，沿着列复制m遍比如A=[1,2,3]，n=2，m=2，那么结果就是[1,2,3,1,2,3 1,2,3,1,2,3]，从一个1*3的矩阵变成一个2*6的矩阵 2.2.4 测试算法：作为完整程序验证分类器上节我们已经将数据做了处理，本节我们将测试分类器的效果。通常我们只提供已有数据的90%作为训练样本来训练分类器，而使用其余的10%数据去测试分类器，检测分类器的正确率；所谓的错误率就是分类器给出的错误结果次数除以测试数据的总数，完美分类器的错误率为0，而错误率为1的分类器不会给出任何正确的结果。代码里我们定义一个计数器变量，每次分类器错误地分类数据，计数器就加1，程序完成之后计数器的结果除以数据点总数就是错误率。为了测试分类器效果，创建函数datingClassTest12345678910111213141516def datingClassTest(): #测试分类器的功能 hoRatio = 0.1#测试集占数据集的比例 datingDataMat,datingLabels = file2matrix('datingTestSet2.txt',3) normMat,ranges,minVals = autoNorm(datingDataMat) m = normMat.shape[0] numTestVecs = int(m*hoRatio) errorCount = 0.0 for i in range(numTestVecs): classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:], datingLabels[numTestVecs:m],3) print (\"the classifier came back with: %d, the real answer is: %d\" % (classifierResult,datingLabels[i])) if classifierResult != datingLabels[i]: errorCount += 1.0 print (\"the total error rate is: %f\" % (errorCount/float(numTestVecs))) 注意此处数据是随机的，所以测试集连续也没关系。再输入datingClassTest()调用该函数，我们可以得到：the classifier came back with: 3, the real answer is: 3the classifier came back with: 2, the real answer is: 2the classifier came back with: 1, the real answer is: 1the classifier came back with: 1, the real answer is: 1…the classifier came back with: 2, the real answer is: 2the classifier came back with: 1, the real answer is: 1the classifier came back with: 3, the real answer is: 1the total error rate is: 0.050000分类器处理约会数据集的错误率大概是5%,这是一个不错的结果。我们可以改变hoRatio变量k的值，可以发现结果可能会大不相同。 2.2.5 使用算法：构建完整可用系统我们会给海伦一小段程序，通过改程序海伦会在约会网站上找到某个人并输入他的信息，程序会反馈回她对对方喜欢程度的预测值。于是我们可以编写出约会网站预测函数:1234567891011def classifyPerson(): #预测相亲对象的满意度 resultList = ['not at all','in small doses','in large doses'] percentTats = float(input(\"percentage oftime spent plating video games?\")) ffMiles = float(input(\"frequent flier miles earned per year?\")) iceCream = float(input(\"liters of ice cream consumed per year?\")) datingDataMat,datingLabels = file2matrix('datingTestSet2.txt',3) normMat,ranges,minVals = autoNorm(datingDataMat) inArr = np.array([ffMiles,percentTats,iceCream]) classifierResult = classify0((inArr-minVals)/ranges,normMat,datingLabels,3) print (\"You will probably like this person: \",resultList[classifierResult - 1]) 再输入classifyPerson(),依次输入10,10000,0.5,可以得到如下结果： percentage oftime spent plating video games?10frequent flier miles earned per year?10000liters of ice cream consumed per year?0.5You will probably like this person: in small doses 2.3 手写识别系统 2.3.1 准备数据：将图像转换为测试向量实际图像存储在第二章源代码的两个子目录内：trainingDigits中包含了大约2000个例子，每个例子的内容如下图所示；目录testDigits中包含了大约900个测试数据。我们使用trainingDigits中的数据训练分类器，使用目录testDigits中的数据测试分类器的效果。两组数据没有重叠。为了使用前面两个例子的分类器，我们必须将图像格式化处理为一个向量。我们将把一个32*32的二进制图像矩阵转换为1*1024的向量，这样前两节使用的分类器就可以处理这个数字图像信息了。我们先编写函数img2vector，将图像转换为向量:该函数创建1*1024的Numpy数组，然后打开给定的文件，循环读出文件的前32行，并将每行的头32个字符值存储在Numpy数组中，最后返回数组。12345678def img2vector(filename): returnVcet = np.zeros((1,1024)) fr = open(filename) for i in range(32): line = fr.readline() for j in range(32): returnVcet[0,i*32+j] = int(line[j]) return returnVcet 2.3.2 测试算法：使用k近邻算法识别手写数字上节我们已经将数据处理成分类器可以识别的格式，本节我们将这些数据输入到分类器，检测分类器的执行效果。在写入以下代码前，我们必须确保将from os import listdir写入文件的起始部分，这段代码的主要功能是从os模块中导入函数listir,他可以列出该目录下所有文件的文件名（方便遍历）123456789101112131415161718192021222324def handwritingClassTest(): hwLabels = [] trainingFileList = listdir('digits/trainingDigits')#获取该文件夹下的所有文件名字 m = len(trainingFileList) trainingMat = np.zeros((m,1024)) for i in range(m): fileNameStr = trainingFileList[i] fileStr = fileNameStr.split('.')[0] classNumStr = int(fileStr.split('_')[0]) hwLabels.append(classNumStr) trainingMat[i,:] = img2vector('digits/trainingDigits/%s' % fileNameStr) testFileList = listdir('digits/testDigits') errorCount = 0.0 mTest = len(testFileList) for i in range(mTest): fileNameStr = testFileList[i] fileStr = fileNameStr.split(',')[0] classNumStr = int(fileStr.split('_')[0]) vector = img2vector('digits/testDigits/%s' % fileNameStr) classifierResult = knn.classify0(vector,trainingMat,hwLabels,3) print (\"the classifier came back with: %d, the real answei is: %d\" % (classifierResult,classNumStr)) if classifierResult != classNumStr: errorCount += 1.0 print (\"the total error rate is: %f\" % (errorCount/float(mTest))) 再输入handwritingClassTest(),等待一会可以得到以下结果:the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4…the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the total error rate is: 0.010571 可以看到k近邻算法识别手写数字数据集，错误率仅为1.2%,可以尝试改变k的值、修改函数handwritingClassTest随机选取训练样本、改变训练样本的数目，都会对k近邻算法的错误率产生影响，感兴趣的话可以改变这些变量值，观察错误率的变化。观察以上算法可以发现实际使用时，算法的效果并不高。算法要为每个测试向量做2000次距离计算，每个距离计算包括了1024个维度浮点运算，总计要执行900次，此外还需要为测试向量准备2MB的存储空间。是否存在一种算法可以减少存储空间和计算时间的开销呢？k决策树（kd树）就是k近邻算法的优化版，可以节省大量的计算开销。 2.4 本章小结k近邻算法是分类数据最简单最有效的算法，k近邻算法必须保存全部数据集，如果训练数据集很大，必须使用大量的存储空间。此外，由于必须对数据集中每个数据计算距离值，实际使用时可能非常耗时。k近邻算法的另一个缺陷是它无法给出任何数据的基础结构信息，因此我们也无法知晓平均实例样本具有什么特征。下一章的决策树可以解决这个问题。 2.5 本章完整代码 knn.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import numpy as npimport matplotlib.pyplot as pltimport osplt.rcParams['font.sans-serif'] = ['SimHei']#用来正常显示中文标签plt.rcParams['axes.unicode_minus'] = False #用来正常显示负号def createDataset(): group = np.array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]]) labels = ['A','A','B','B'] return group,labelsdef classify0(inx,dataSet,labels,k): #knn,inx是输入向量，dataSet和labels是训练集，k是knn的k dataSetSize = dataSet.shape diffMat = np.tile(inx,(dataSetSize[0],1)) - dataSet#np.tile函数是表示把inx沿着行列方向复制几次 sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1)#axis表示求每一行的和 distances = sqDistances ** 0.5 sortDistIndicies = distances.argsort() classCount = &#123;&#125; for i in range(k): label = labels[sortDistIndicies[i]] classCount[label] = classCount.get(label,0) + 1#get方法返回对应的值，若不存在返回默认值0 sortClassCount = sorted(classCount.items(),key=lambda d:d[1],reverse=True)#reverse表示从大到小 return sortClassCount[0][0]def file2matrix(filename,n): #读取文件，返回特征矩阵和标签列表 file = open(filename) arrayOLines = file.readlines() numberOfLines = len(arrayOLines) returnMat = np.zeros((numberOfLines,n)) classLabelList = [] index = 0 for line in arrayOLines: line = line.strip() line = line.split('\\t') returnMat[index,:] = line[0:n] classLabelList.append(int(line[-1])) index += 1 return returnMat,classLabelListdef autoNorm(dataSet): #归一化特征 minVals = dataSet.min(0) maxVals = dataSet.max(0) ranges = maxVals - minVals m = dataSet.shape[0] normDataSet = dataSet - np.tile(minVals,(m,1)) normDataSet = normDataSet/np.tile(ranges,(m,1))#对应位置相除，不是矩阵除法 return normDataSet,ranges,minValsdef datingClassTest(): #测试分类器的功能 hoRatio = 0.1#测试集占数据集的比例 datingDataMat,datingLabels = file2matrix('datingTestSet2.txt',3) normMat,ranges,minVals = autoNorm(datingDataMat) m = normMat.shape[0] numTestVecs = int(m*hoRatio) errorCount = 0.0 for i in range(numTestVecs): classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:], datingLabels[numTestVecs:m],3) print (\"the classifier came back with: %d, the real answer is: %d\" % (classifierResult,datingLabels[i])) if classifierResult != datingLabels[i]: errorCount += 1.0 print (\"the total error rate is: %f\" % (errorCount/float(numTestVecs))) def classifyPerson(): #预测相亲对象的满意度 resultList = ['not at all','in small doses','in large doses'] percentTats = float(input(\"percentage oftime spent plating video games?\")) ffMiles = float(input(\"frequent flier miles earned per year?\")) iceCream = float(input(\"liters of ice cream consumed per year?\")) datingDataMat,datingLabels = file2matrix('datingTestSet2.txt',3) normMat,ranges,minVals = autoNorm(datingDataMat) inArr = np.array([ffMiles,percentTats,iceCream]) classifierResult = classify0((inArr-minVals)/ranges,normMat,datingLabels,3) print (\"You will probably like this person: \",resultList[classifierResult - 1]) #classifyPerson()#datingClassTest() find_digit.py(手写数字识别) 123456789101112131415161718192021222324252627282930313233343536373839import knnimport numpy as npfrom os import listdirdef img2vector(filename): returnVcet = np.zeros((1,1024)) fr = open(filename) for i in range(32): line = fr.readline() for j in range(32): returnVcet[0,i*32+j] = int(line[j]) return returnVcetdef handwritingClassTest(): hwLabels = [] trainingFileList = listdir('digits/trainingDigits')#获取该文件夹下的所有文件名字 m = len(trainingFileList) trainingMat = np.zeros((m,1024)) for i in range(m): fileNameStr = trainingFileList[i] fileStr = fileNameStr.split('.')[0] classNumStr = int(fileStr.split('_')[0]) hwLabels.append(classNumStr) trainingMat[i,:] = img2vector('digits/trainingDigits/%s' % fileNameStr) testFileList = listdir('digits/testDigits') errorCount = 0.0 mTest = len(testFileList) for i in range(mTest): fileNameStr = testFileList[i] fileStr = fileNameStr.split(',')[0] classNumStr = int(fileStr.split('_')[0]) vector = img2vector('digits/testDigits/%s' % fileNameStr) classifierResult = knn.classify0(vector,trainingMat,hwLabels,3) print (\"the classifier came back with: %d, the real answei is: %d\" % (classifierResult,classNumStr)) if classifierResult != classNumStr: errorCount += 1.0 print (\"the total error rate is: %f\" % (errorCount/float(mTest)))handwritingClassTest() 后记啊，终于写完了，真累，希望这个系列可以坚持到最后吧（逃感谢观看，最后放张灵梦镇博（笑","tags":[{"name":"机器学习实战","slug":"机器学习实战","permalink":"http://flandre.site/tags/机器学习实战/"},{"name":"机器学习","slug":"机器学习","permalink":"http://flandre.site/tags/机器学习/"}]},{"title":"重新开始更新博客","date":"2018-05-12T12:22:35.078Z","path":"2018/05/12/重新开始更新博客/","text":"很久没有好好更新过博客了，想着是时候好好更新一波了之前的博客由于图片大部分失效，想着也没什么有含金量的内容，就全删了，so，再一次从零开始本人CSDN：https://my.csdn.net/acm_cxq","tags":[{"name":"杂谈","slug":"杂谈","permalink":"http://flandre.site/tags/杂谈/"}]},{"title":"解决markdown无法显示本地图片的办法","date":"2017-12-29T12:16:31.598Z","path":"2017/12/29/解决markdown无法显示本地图片的办法/","text":"如果觉得这个方法麻烦可以试一下QQ空间相册之类的图床，应该还是不用怕被删除的。参考 http://blog.csdn.net/sugar_rainbow/article/details/57415705这里再说一下重点： 把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true 在你的hexo目录下执行这样一句话npm install hexo-asset-image –save 等待一小段时间后，再运行hexo n “xxxx”来生成md博文时，/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹当然直接在_posts文件夹里面创建一个跟你博文同名的文件夹也可以 最后在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片：注意： xxxx是这个md文件的名字，也是同名文件夹的名字。只需要有文件夹名字即可，不需要有什么绝对路径。你想引入的图片就只需要放入xxxx这个文件夹内就好了，很像引用相对路径。","tags":[{"name":"markdown","slug":"markdown","permalink":"http://flandre.site/tags/markdown/"}]}]