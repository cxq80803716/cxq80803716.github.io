[{"title":"【机器学习实战】第三章 决策树","date":"2018-05-13T13:33:53.940Z","path":"2018/05/13/【机器学习实战】第三章 决策树/","text":"序言本人的环境是Anaconda3.5，实际运行过程中发现书中不少代码需要小改动，所以如果我的代码和书中不同，也是正常的注意：本博客并非完全照抄书上内容（太多了），只是摘选出本人觉得对学习比较有用的内容+一点个人的理解，并且保证代码是可运行的关于该书的源代码和数据集： 链接：点我 密码：rh3a 这篇博客主要介绍的是这本书的第三章: 决策树 如果你以前没有接触过决策树，完全不用担心，它的概念非常简单。如下图就是一个决策树，分支节点就是决策用的某一个特征值，叶子结点表示已经得出结果。可想而知，当我们构造出这样一颗决策树，对于一个未知结果的输入向量，能够很快得到预测结果。专家系统中经常使用决策树，而且决策树给出结果往往可以匹敌在当前领域具有几十年工作经验的人类专家。决策树的优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。缺点：可能会产生过度匹配问题使用数据类型：数值型和标称型 3.1 决策树的构造在构造决策树时，我们需要解决的第一个问题就是当前数据集上哪个特征在划分数据分类时起决定性作用，也就是说选择这个特征值可以使决策树更小。为了找到决定性的特征，划分出最好的结果，我们必须评估每一个特征。完成测试之后，原始数据集就被划分成几个数据子集。这些数据子集会分布在第一个决策点的所有分支上。如果某个分支下的数据属于同一类型，则当前已经正确划分数据分类；无需进一步对数据集进行分割。如果数据子集内的数据不属于同一类型，则需要重复划分数据子集。划分数据子集的算法和划分原始数据集的方法相同，直到所有具有相同类型的数据均在一个数据子集内。 3.1.1 信息增益划分数据集的大原则是：将无序的数据变得更加有序。我们可以使用多种方法划分数据集，但是每种方法都有各自的优缺点。组织杂乱无章数据的一种方法就是使用信息论度量信息。在划分数据集之前之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。什么是熵？熵定义为信息的期望值，在明晰这个概念之前，我们必须知道信息的定义。如果待分类的事物可能划分在多个分类之中，则符号xi的信息定义为其中p(xi)是选择分类xi的概率，显然信息量的取值范围在(0,+∞)为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，可以通过下面的公式得到：其中n是分类的数目，从公式可以看出这个信息熵相当于所有分类的期望熵，由此公式我们可以计算给定数据集的熵，代码如下：12345678910111213141516from math import logdef calcShannonEnt(dataSet): #计算数据集的香农熵，表征混乱程度 numEntries = len(dataSet) labelCounts = &#123;&#125;#该数据集所有分类出现的次数 for featVec in dataSet: currentLabel = featVec[-1] if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 labelCounts[currentLabel] += 1 shannonEnt = 0.0 for key in labelCounts: prob = float(labelCounts[key])/numEntries shannonEnt -= prob * log(prob,2) return shannonEnt 为了测试这个函数的功能，我们可以利用createDataSet()函数得到简单的鱼类鉴定数据集12345def createDataSet(): #创建数据集 dataSet = [[1,1,'yes'],[1,1,'yes'],[1,0,'no'],[0,1,'no'],[0,1,'no']] labels = ['no surfacing','flippers']#两个特征的含义 return dataSet,labels 再输入：myDat,labels = createDataSet()print (myDat)print (calcShannonEnt(myDat))我们可以得到如下结果：[[1, 1, ‘yes’], [1, 1, ‘yes’], [1, 0, ‘no’], [0, 1, ‘no’], [0, 1, ‘no’]]0.9709505944546686熵越高，则混合的数据也越多，数据更加无序。得到熵之后，我们就可以按照获取最大信息增益的方法划分数据集，下一节我们将具体学习如何划分数据集以及如何度量信息增益。 3.1.2 划分数据集上节我们学习了如何度量数据集的无序程度。我们将对每个特征划分数据集的结果计算一次信息熵，然后判断按照哪个特征划分数据集是最好的划分方式。123456789def splitDataSet(dataSet,axis,value): #返回第axis个特征为value的的数据集，要去掉第axis个特征 retDataSet = [] for featVec in dataSet: if featVec[axis] == value: reducedFeatVec = featVec[:axis] reducedFeatVec.extend(featVec[axis+1:]) retDataSet.append(reducedFeatVec) return retDataSet 以上代码使用了三个输入参数：待划分的数据集、目标特征的下标，目标特征。注意区分append和extend的区别假如a = [1,2,3],b = [4,5,6]则a.append(b)的结果是[1,2,3,[4,5,6]]a.extend(b)的结果是[1,2,3,4,5,6]我们可以在前面的简单样本数据上测试该函数输入:myDat,labels = createDataSet()print (myDat)print (splitDataSet(myDat,0,1))print (splitDataSet(myDat,0,0))我们可以得到：[[1, 1, ‘yes’], [1, 1, ‘yes’], [1, 0, ‘no’], [0, 1, ‘no’], [0, 1, ‘no’]][[1, ‘yes’], [1, ‘yes’], [0, ‘no’]][[1, ‘no’], [1, ‘no’]]我们便用第一个特征对原数据集进行了划分，分成了两个子集接下来我们将遍历整个数据集，循环计算香农熵和splitDataSet()函数，找到最好的特征划分方式。123456789101112131415161718def chooseBestFeatureToSplit(dataSet): #选择当前分类的最佳特征，即信息增益最大的特征 numFeature = len(dataSet[0]) - 1 baseEntropy = calcShannonEnt(dataSet) bestInfoGain = 0.0;bestFeature = -1 for i in range(numFeature):#遍历所有特征 featList = [example[i] for example in dataSet] UniqueFeatList = set(featList) newEntropy = 0.0 for value in UniqueFeatList:#遍历该特征的所有取值，求出平均香农熵 subDataSet = splitDataSet(dataSet,i,value) prob = len(subDataSet) / float(len(dataSet)) newEntropy += prob * calcShannonEnt(subDataSet) infoGain = baseEntropy - newEntropy if infoGain &gt; bestInfoGain:#松弛操作 bestInfoGain = infoGain bestFeature = i return bestFeature","tags":[{"name":"机器学习实战","slug":"机器学习实战","permalink":"http://flandre.site/tags/机器学习实战/"},{"name":"机器学习","slug":"机器学习","permalink":"http://flandre.site/tags/机器学习/"}]},{"title":"【机器学习实战】第二章 k近邻算法","date":"2018-05-13T07:37:50.550Z","path":"2018/05/13/【机器学习实战】第二章 k近邻算法/","text":"序言马上就要毕业了，研究生的方向是机器学习和深度学习最近开始学习机器学习，刷完吴恩达的视频后想着应该拿本实战书练手理论+实践才能出真知然后就开始啃这本书，现在看了三章，感觉这本书写的还是相当不错的，简单易懂，能实现一些基本的功能本人的环境是Anaconda3.5，实际运行过程中发现书中不少代码需要小改动，所以如果我的代码和书中不同，也是正常的注意：本博客并非完全照抄书上内容（太多了），只是摘选出本人觉得对学习比较有用的内容+一点个人的理解，并且保证代码是可运行的关于该书的源代码和数据集： 链接：点我 密码：rh3a 这篇博客主要介绍的是这本书的第二章: k近邻算法 2.1 k近邻算法概述 工作原理存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中数据最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k近邻算法中k的出处，通常k&lt;=20。最后选择k个最相似数据中出现次数最多的分类，作为新数据的分类。 k近邻算法 1234567891011121314def classify0(inx,dataSet,labels,k): #knn,inx是输入向量，dataSet和labels是训练集，k是knn的k dataSetSize = dataSet.shape diffMat = np.tile(inx,(dataSetSize[0],1)) - dataSet#np.tile函数是表示把inx沿着行列方向复制几次 sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1)#axis表示求每一行的和 distances = sqDistances ** 0.5 sortDistIndicies = distances.argsort() classCount = &#123;&#125; for i in range(k): label = labels[sortDistIndicies[i]] classCount[label] = classCount.get(label,0) + 1#get方法返回对应的值，若不存在返回默认值0 sortClassCount = sorted(classCount.items(),key=lambda d:d[1],reverse=True)#reverse表示从大到小 return sortClassCount[0][0] classify0用于实现k近邻算法，有4个输入参数：用于分类的输入向量是inX，输入的训练样本集为dataSet，标签向量为labels，最后的参数k表示最近邻居的数目这里关于两个数据之间距离的计算，我们采用欧几里得距离:例如数据集存在4个特征值，则点（7,6,9,4）与（1,0,0,1）之间的距离计算为：计算完所有点之间的距离后，可以对数据按照从小到大的次序排序。然后，确定前k个距离最小元素所在的主要分类，输入k总是正整数；之后将classCount字典分解为元组列表，然后使用lambda内置函数进行排序，注意这里d是一个键值对,d[1]指的就是值,reverse表示此处的排序为逆序，最后返回发生频率最高的元素标签。 2.2 示例: 使用k近邻算法改进约会网站的配对效果背景：我的朋友海伦一直使用在线约会网站寻找适合自己的约会对象，她经过总结发现她曾交往过三种类型的人：（1）不喜欢的人（2）魅力一般的人（3）极具魅力的人海伦希望我们的分类软件可以更好的帮助她将匹配对象划分到确切的分类中 海伦收集约会数据已经有一段时间了，她把这些数据存放在文本文件datingTestSet2.txt中，每个样本数据占据一行，总共有1000行。海伦的样本主要包含以下3种特征：（1）每年获得的飞行常客里程数（2）玩视频游戏所耗时间百分比（3）每周消费的冰淇淋公升数 2.2.1 准备数据:从文本文件中解析数据在将上述特征数据输入到分类器之前，必须将待处理数据的格式改变为分类器可以接受的格式。于是创建file2matrix的函数，以此来处理输入格式问题，该函数输入为文件名字符串，输出为训练样本矩阵和类标签向量。123456789101112131415def file2matrix(filename,n): #读取文件，返回特征矩阵和标签列表 file = open(filename) arrayOLines = file.readlines() numberOfLines = len(arrayOLines) returnMat = np.zeros((numberOfLines,n)) classLabelList = [] index = 0 for line in arrayOLines: line = line.strip() line = line.split('\\t') returnMat[index,:] = line[0:n] classLabelList.append(int(line[-1])) index += 1 return returnMat,classLabelList 2.2.2 分析数据：使用Matplotlib创建散点图 2.2.3 准备数据：归一化数据如果有两组特征数据：（0,20000,1.1）和（67,32000,0.1）那么他们之间的距离为：容易发现，上面方程中数字差值最大的属性对计算结果的影响最大，也即是说，每年获取的飞行常客里程数（此处定为第二个特征）对于计算结果的影响远远大于其他两个特征。但海伦认为这三个特征是同等重要的，因此我们要对数据做归一化处理。说起归一化，我们容易想到将取值范围处理到0~1或者-1~1之间，比如下面这个公式：newValue = (oldValue-min)/(max-min)于是我们可以编写出归一化函数autoNorm的代码：123456789def autoNorm(dataSet): #归一化特征 minVals = dataSet.min(0) maxVals = dataSet.max(0) ranges = maxVals - minVals m = dataSet.shape[0] normDataSet = dataSet - np.tile(minVals,(m,1)) normDataSet = normDataSet/np.tile(ranges,(m,1))#对应位置的值相除，不是矩阵除法 return normDataSet,ranges,minVals 这里解释一下np.tile(A,(n,m))函数,它的意思是把A沿着行复制n遍，沿着列复制m遍比如A=[1,2,3]，n=2，m=2，那么结果就是[1,2,3,1,2,3 1,2,3,1,2,3]，从一个1*3的矩阵变成一个2*6的矩阵 2.2.4 测试算法：作为完整程序验证分类器上节我们已经将数据做了处理，本节我们将测试分类器的效果。通常我们只提供已有数据的90%作为训练样本来训练分类器，而使用其余的10%数据去测试分类器，检测分类器的正确率；所谓的错误率就是分类器给出的错误结果次数除以测试数据的总数，完美分类器的错误率为0，而错误率为1的分类器不会给出任何正确的结果。代码里我们定义一个计数器变量，每次分类器错误地分类数据，计数器就加1，程序完成之后计数器的结果除以数据点总数就是错误率。为了测试分类器效果，创建函数datingClassTest12345678910111213141516def datingClassTest(): #测试分类器的功能 hoRatio = 0.1#测试集占数据集的比例 datingDataMat,datingLabels = file2matrix('datingTestSet2.txt',3) normMat,ranges,minVals = autoNorm(datingDataMat) m = normMat.shape[0] numTestVecs = int(m*hoRatio) errorCount = 0.0 for i in range(numTestVecs): classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:], datingLabels[numTestVecs:m],3) print (\"the classifier came back with: %d, the real answer is: %d\" % (classifierResult,datingLabels[i])) if classifierResult != datingLabels[i]: errorCount += 1.0 print (\"the total error rate is: %f\" % (errorCount/float(numTestVecs))) 注意此处数据是随机的，所以测试集连续也没关系。再输入datingClassTest()调用该函数，我们可以得到：the classifier came back with: 3, the real answer is: 3the classifier came back with: 2, the real answer is: 2the classifier came back with: 1, the real answer is: 1the classifier came back with: 1, the real answer is: 1…the classifier came back with: 2, the real answer is: 2the classifier came back with: 1, the real answer is: 1the classifier came back with: 3, the real answer is: 1the total error rate is: 0.050000分类器处理约会数据集的错误率大概是5%,这是一个不错的结果。我们可以改变hoRatio变量k的值，可以发现结果可能会大不相同。 2.2.5 使用算法：构建完整可用系统我们会给海伦一小段程序，通过改程序海伦会在约会网站上找到某个人并输入他的信息，程序会反馈回她对对方喜欢程度的预测值。于是我们可以编写出约会网站预测函数:1234567891011def classifyPerson(): #预测相亲对象的满意度 resultList = ['not at all','in small doses','in large doses'] percentTats = float(input(\"percentage oftime spent plating video games?\")) ffMiles = float(input(\"frequent flier miles earned per year?\")) iceCream = float(input(\"liters of ice cream consumed per year?\")) datingDataMat,datingLabels = file2matrix('datingTestSet2.txt',3) normMat,ranges,minVals = autoNorm(datingDataMat) inArr = np.array([ffMiles,percentTats,iceCream]) classifierResult = classify0((inArr-minVals)/ranges,normMat,datingLabels,3) print (\"You will probably like this person: \",resultList[classifierResult - 1]) 再输入classifyPerson(),依次输入10,10000,0.5,可以得到如下结果： percentage oftime spent plating video games?10frequent flier miles earned per year?10000liters of ice cream consumed per year?0.5You will probably like this person: in small doses 2.3 手写识别系统 2.3.1 准备数据：将图像转换为测试向量实际图像存储在第二章源代码的两个子目录内：trainingDigits中包含了大约2000个例子，每个例子的内容如下图所示；目录testDigits中包含了大约900个测试数据。我们使用trainingDigits中的数据训练分类器，使用目录testDigits中的数据测试分类器的效果。两组数据没有重叠。为了使用前面两个例子的分类器，我们必须将图像格式化处理为一个向量。我们将把一个32*32的二进制图像矩阵转换为1*1024的向量，这样前两节使用的分类器就可以处理这个数字图像信息了。我们先编写函数img2vector，将图像转换为向量:该函数创建1*1024的Numpy数组，然后打开给定的文件，循环读出文件的前32行，并将每行的头32个字符值存储在Numpy数组中，最后返回数组。12345678def img2vector(filename): returnVcet = np.zeros((1,1024)) fr = open(filename) for i in range(32): line = fr.readline() for j in range(32): returnVcet[0,i*32+j] = int(line[j]) return returnVcet 2.3.2 测试算法：使用k近邻算法识别手写数字上节我们已经将数据处理成分类器可以识别的格式，本节我们将这些数据输入到分类器，检测分类器的执行效果。在写入以下代码前，我们必须确保将from os import listdir写入文件的起始部分，这段代码的主要功能是从os模块中导入函数listir,他可以列出该目录下所有文件的文件名（方便遍历）123456789101112131415161718192021222324def handwritingClassTest(): hwLabels = [] trainingFileList = listdir('digits/trainingDigits')#获取该文件夹下的所有文件名字 m = len(trainingFileList) trainingMat = np.zeros((m,1024)) for i in range(m): fileNameStr = trainingFileList[i] fileStr = fileNameStr.split('.')[0] classNumStr = int(fileStr.split('_')[0]) hwLabels.append(classNumStr) trainingMat[i,:] = img2vector('digits/trainingDigits/%s' % fileNameStr) testFileList = listdir('digits/testDigits') errorCount = 0.0 mTest = len(testFileList) for i in range(mTest): fileNameStr = testFileList[i] fileStr = fileNameStr.split(',')[0] classNumStr = int(fileStr.split('_')[0]) vector = img2vector('digits/testDigits/%s' % fileNameStr) classifierResult = knn.classify0(vector,trainingMat,hwLabels,3) print (\"the classifier came back with: %d, the real answei is: %d\" % (classifierResult,classNumStr)) if classifierResult != classNumStr: errorCount += 1.0 print (\"the total error rate is: %f\" % (errorCount/float(mTest))) 再输入handwritingClassTest(),等待一会可以得到以下结果:the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4the classifier came back with: 4, the real answei is: 4…the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the classifier came back with: 9, the real answei is: 9the total error rate is: 0.010571 可以看到k近邻算法识别手写数字数据集，错误率仅为1.2%,可以尝试改变k的值、修改函数handwritingClassTest随机选取训练样本、改变训练样本的数目，都会对k近邻算法的错误率产生影响，感兴趣的话可以改变这些变量值，观察错误率的变化。观察以上算法可以发现实际使用时，算法的效果并不高。算法要为每个测试向量做2000次距离计算，每个距离计算包括了1024个维度浮点运算，总计要执行900次，此外还需要为测试向量准备2MB的存储空间。是否存在一种算法可以减少存储空间和计算时间的开销呢？k决策树（kd树）就是k近邻算法的优化版，可以节省大量的计算开销。 2.4 本章小结k近邻算法是分类数据最简单最有效的算法，k近邻算法必须保存全部数据集，如果训练数据集很大，必须使用大量的存储空间。此外，由于必须对数据集中每个数据计算距离值，实际使用时可能非常耗时。k近邻算法的另一个缺陷是它无法给出任何数据的基础结构信息，因此我们也无法知晓平均实例样本具有什么特征。下一章的决策树可以解决这个问题。 2.5 本章完整代码 knn.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import numpy as npimport matplotlib.pyplot as pltimport osplt.rcParams['font.sans-serif'] = ['SimHei']#用来正常显示中文标签plt.rcParams['axes.unicode_minus'] = False #用来正常显示负号def createDataset(): group = np.array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]]) labels = ['A','A','B','B'] return group,labelsdef classify0(inx,dataSet,labels,k): #knn,inx是输入向量，dataSet和labels是训练集，k是knn的k dataSetSize = dataSet.shape diffMat = np.tile(inx,(dataSetSize[0],1)) - dataSet#np.tile函数是表示把inx沿着行列方向复制几次 sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1)#axis表示求每一行的和 distances = sqDistances ** 0.5 sortDistIndicies = distances.argsort() classCount = &#123;&#125; for i in range(k): label = labels[sortDistIndicies[i]] classCount[label] = classCount.get(label,0) + 1#get方法返回对应的值，若不存在返回默认值0 sortClassCount = sorted(classCount.items(),key=lambda d:d[1],reverse=True)#reverse表示从大到小 return sortClassCount[0][0]def file2matrix(filename,n): #读取文件，返回特征矩阵和标签列表 file = open(filename) arrayOLines = file.readlines() numberOfLines = len(arrayOLines) returnMat = np.zeros((numberOfLines,n)) classLabelList = [] index = 0 for line in arrayOLines: line = line.strip() line = line.split('\\t') returnMat[index,:] = line[0:n] classLabelList.append(int(line[-1])) index += 1 return returnMat,classLabelListdef autoNorm(dataSet): #归一化特征 minVals = dataSet.min(0) maxVals = dataSet.max(0) ranges = maxVals - minVals m = dataSet.shape[0] normDataSet = dataSet - np.tile(minVals,(m,1)) normDataSet = normDataSet/np.tile(ranges,(m,1))#对应位置相除，不是矩阵除法 return normDataSet,ranges,minValsdef datingClassTest(): #测试分类器的功能 hoRatio = 0.1#测试集占数据集的比例 datingDataMat,datingLabels = file2matrix('datingTestSet2.txt',3) normMat,ranges,minVals = autoNorm(datingDataMat) m = normMat.shape[0] numTestVecs = int(m*hoRatio) errorCount = 0.0 for i in range(numTestVecs): classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:], datingLabels[numTestVecs:m],3) print (\"the classifier came back with: %d, the real answer is: %d\" % (classifierResult,datingLabels[i])) if classifierResult != datingLabels[i]: errorCount += 1.0 print (\"the total error rate is: %f\" % (errorCount/float(numTestVecs))) def classifyPerson(): #预测相亲对象的满意度 resultList = ['not at all','in small doses','in large doses'] percentTats = float(input(\"percentage oftime spent plating video games?\")) ffMiles = float(input(\"frequent flier miles earned per year?\")) iceCream = float(input(\"liters of ice cream consumed per year?\")) datingDataMat,datingLabels = file2matrix('datingTestSet2.txt',3) normMat,ranges,minVals = autoNorm(datingDataMat) inArr = np.array([ffMiles,percentTats,iceCream]) classifierResult = classify0((inArr-minVals)/ranges,normMat,datingLabels,3) print (\"You will probably like this person: \",resultList[classifierResult - 1]) #classifyPerson()#datingClassTest() find_digit.py(手写数字识别) 123456789101112131415161718192021222324252627282930313233343536373839import knnimport numpy as npfrom os import listdirdef img2vector(filename): returnVcet = np.zeros((1,1024)) fr = open(filename) for i in range(32): line = fr.readline() for j in range(32): returnVcet[0,i*32+j] = int(line[j]) return returnVcetdef handwritingClassTest(): hwLabels = [] trainingFileList = listdir('digits/trainingDigits')#获取该文件夹下的所有文件名字 m = len(trainingFileList) trainingMat = np.zeros((m,1024)) for i in range(m): fileNameStr = trainingFileList[i] fileStr = fileNameStr.split('.')[0] classNumStr = int(fileStr.split('_')[0]) hwLabels.append(classNumStr) trainingMat[i,:] = img2vector('digits/trainingDigits/%s' % fileNameStr) testFileList = listdir('digits/testDigits') errorCount = 0.0 mTest = len(testFileList) for i in range(mTest): fileNameStr = testFileList[i] fileStr = fileNameStr.split(',')[0] classNumStr = int(fileStr.split('_')[0]) vector = img2vector('digits/testDigits/%s' % fileNameStr) classifierResult = knn.classify0(vector,trainingMat,hwLabels,3) print (\"the classifier came back with: %d, the real answei is: %d\" % (classifierResult,classNumStr)) if classifierResult != classNumStr: errorCount += 1.0 print (\"the total error rate is: %f\" % (errorCount/float(mTest)))handwritingClassTest() 后记啊，终于写完了，真累，希望这个系列可以坚持到最后吧（逃感谢观看，最后放张灵梦镇博（笑","tags":[{"name":"机器学习实战","slug":"机器学习实战","permalink":"http://flandre.site/tags/机器学习实战/"},{"name":"机器学习","slug":"机器学习","permalink":"http://flandre.site/tags/机器学习/"}]},{"title":"重新开始更新博客","date":"2018-05-12T12:22:35.078Z","path":"2018/05/12/重新开始更新博客/","text":"很久没有好好更新过博客了，想着是时候好好更新一波了之前的博客由于图片大部分失效，想着也没什么有含金量的内容，就全删了，so，再一次从零开始本人CSDN：https://my.csdn.net/acm_cxq","tags":[{"name":"杂谈","slug":"杂谈","permalink":"http://flandre.site/tags/杂谈/"}]},{"title":"解决markdown无法显示本地图片的办法","date":"2017-12-29T12:16:31.598Z","path":"2017/12/29/解决markdown无法显示本地图片的办法/","text":"如果觉得这个方法麻烦可以试一下QQ空间相册之类的图床，应该还是不用怕被删除的。参考 http://blog.csdn.net/sugar_rainbow/article/details/57415705这里再说一下重点： 把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true 在你的hexo目录下执行这样一句话npm install hexo-asset-image –save 等待一小段时间后，再运行hexo n “xxxx”来生成md博文时，/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹当然直接在_posts文件夹里面创建一个跟你博文同名的文件夹也可以 最后在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片：注意： xxxx是这个md文件的名字，也是同名文件夹的名字。只需要有文件夹名字即可，不需要有什么绝对路径。你想引入的图片就只需要放入xxxx这个文件夹内就好了，很像引用相对路径。","tags":[{"name":"markdown","slug":"markdown","permalink":"http://flandre.site/tags/markdown/"}]}]